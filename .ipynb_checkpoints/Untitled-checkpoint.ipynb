{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "from misc import *\n",
    "from collections import deque\n",
    "from nets import Q1\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(q1, betas, ep=1):\n",
    "    if np.random.random()<ep:\n",
    "        l = np.random.choice(range(len(betas)), 1)[0]\n",
    "        return l, betas[l]\n",
    "    else:\n",
    "\n",
    "        qs= np.squeeze(q1(np.expand_dims(betas, axis=1)).numpy())\n",
    "        l=np.where(qs==max(qs))[0][0]\n",
    "        return l, betas[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_training(run_id, number_betas=10, lr = 10**-2, ep=10**-2, T=1000):\n",
    "\n",
    "    q1=Q1()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr = lr)\n",
    "    pt=[]\n",
    "    rt=[]\n",
    "    rts = []\n",
    "    \n",
    "    \n",
    "    betas = np.arange(-1,0,1/number_betas)\n",
    "        ntable=np.zeros(len(betas))\n",
    "\n",
    "    optimal = max(ps(betas))\n",
    "    buffer = ReplayBuffer(buffer_size=100)\n",
    "    for episode in tqdm(range(T)):\n",
    "        ep = max(np.exp(-episode/100),0.1)\n",
    "        label, beta = greedy_action(q1, betas, ep)\n",
    "        ntable[label]+=1\n",
    "        reward = np.random.choice([1.,0.],1, p=[ps(beta), 1-ps(beta)])[0]\n",
    "        rt.append(reward)\n",
    "        rts.append(np.sum(rt))\n",
    "        buffer.add(beta, reward)\n",
    "        if episode > 100:\n",
    "            actions_did, rewards = buffer.sample(100)\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(q1.trainable_variables)\n",
    "                    predictions = q1(np.expand_dims(np.array(actions_did),axis=1))\n",
    "                    pt.append(ps(greedy_action(q1,betas,0)[1]))\n",
    "                    loss_sum = tf.keras.losses.MSE(predictions,np.expand_dims(np.array(rewards),axis=1))\n",
    "                    loss = tf.reduce_mean(loss_sum)\n",
    "                    grads = tape.gradient(loss, q1.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(grads, q1.trainable_variables))\n",
    "        else:\n",
    "            pt.append(0.5)\n",
    "    rtsum = rts/np.arange(1,T+1)\n",
    "    predictions = q1.prediction(betas)\n",
    "    plot_evolution(rtsum, pt, optimal, betas, predictions , ntable, run_id)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolution(rt, pt, optimal, betas, preds, ntable, run_id):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    T=len(rt)\n",
    "    ax1=plt.subplot2grid((2,2),(0,0))\n",
    "    ax2=plt.subplot2grid((2,2),(1,0))\n",
    "    ax3=plt.subplot2grid((2,2),(0,1), rowspan=2)\n",
    "\n",
    "    ax1.plot(np.arange(1,T+1),rt, color=\"red\", linewidth=7, alpha=0.8, label=r'$R_t$')\n",
    "    ax1.plot(optimal*np.ones(T), color=\"black\",  linewidth=7,alpha=0.5, label=\"optimal\")\n",
    "    ax2.plot(np.arange(1,T+1),pt, color=\"red\", linewidth=7, alpha=0.8, label=r'$P_t$')\n",
    "    ax2.plot(optimal*np.ones(T), color=\"black\",  linewidth=7,alpha=0.5, label=\"optimal\")\n",
    "    ax3.scatter(betas, preds, color=\"red\", s=30, label=\"predictions\", alpha=0.6)\n",
    "    ax3.scatter(betas, ps(betas), color=\"blue\", s=30, label=\"true values\", alpha=0.6)\n",
    "\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.legend(prop={\"size\":30})\n",
    "    plt.savefig(run_id+\"/learning_curves.png\")\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:02<00:00, 100.19it/s]\n"
     ]
    }
   ],
   "source": [
    "run_id=record()\n",
    "number_run = \"run_\"+str(run_id)\n",
    "real_training(number_run, T=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cooper-cooper/Desktop/deeper'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
