{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import basics\n",
    "import misc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import random\n",
    "\n",
    "basic = basics.Basics(resolution=.1)\n",
    "basic.define_actions()\n",
    "actions = basic.actions\n",
    "ats = misc.make_attenuations(layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._samples = []\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self._max_memory:\n",
    "            self._samples.pop(0)\n",
    "    def sample(self, no_samples):\n",
    "        if no_samples > len(self._samples):\n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else:\n",
    "            return random.sample(self._samples, no_samples)\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinality_betas = len(basic.actions[0])\n",
    "\n",
    "class QN_l1(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_l1,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(0,), kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "        self.l2 = Dense(35, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "\n",
    "        self.l3 = Dense(cardinality_betas, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "class QN_l2(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_l2,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(1,2), kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "        self.l2 = Dense(35, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "\n",
    "\n",
    "        self.l3 = Dense(cardinality_betas, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "class QN_guess(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_guess,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(1,4), kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l2 = Dense(35, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "        self.l3 = Dense(2, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "\n",
    "#### define the networks #####\n",
    "\n",
    "qn_l1_prim = QN_l1()\n",
    "qn_l1_targ = QN_l1()\n",
    "\n",
    "qn_l2_prim = QN_l2()\n",
    "qn_l2_targ = QN_l2()\n",
    "\n",
    "qn_guess_prim = QN_guess()\n",
    "qn_guess_targ = QN_guess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_first_beta(epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        label = np.random.choice(np.arange(len(basic.actions[0])))\n",
    "        return label, basic.actions[0][label]\n",
    "    else:\n",
    "        input = np.expand_dims(np.array([]), axis=0)\n",
    "        q1s = qn_l1_prim(input)\n",
    "        q1s = q1s.numpy()\n",
    "        label = np.argmax(q1s)\n",
    "        beta1 = basic.actions[0][label]\n",
    "        return label, beta1\n",
    "\n",
    "def give_second_beta(new_state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        label = np.random.choice(np.arange(len(basic.actions[1])))\n",
    "        return label, basic.actions[1][label]\n",
    "    else:\n",
    "        input = np.expand_dims(np.array(new_state), axis=0)\n",
    "        q2s = qn_l2_prim(input)\n",
    "        q2s = q2s.numpy()\n",
    "        label = np.argmax(q2s)\n",
    "        beta2 = basic.actions[1][label]\n",
    "        return label, beta2\n",
    "\n",
    "\n",
    "def give_guess(new_state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        guess = np.random.choice(basic.possible_phases,1)[0]\n",
    "        return int((guess+1)/2), guess\n",
    "    else:\n",
    "        input = np.expand_dims(np.array(new_state), axis=0)\n",
    "        qguess = qn_guess_prim(input)\n",
    "        guess = qguess.numpy()\n",
    "        label = np.argmax(guess)\n",
    "        guess = basic.possible_phases[label]\n",
    "        return int((guess+1)/2), guess\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    batch_length=32\n",
    "    batch = buffer.sample(batch_length)\n",
    "\n",
    "    s_2_batch = np.array([[ v[0], v[2]] for v in batch ] )\n",
    "    labels_beta1 = np.array([v[4] for v in batch])\n",
    "\n",
    "    q_2_prim = qn_l2_prim(np.expand_dims(s_2_batch, axis=0))\n",
    "    q_2_prim = np.squeeze(q_2_prim.numpy())\n",
    "\n",
    "    opt_a_2_prim = np.argmax(q_2_prim,axis=1)\n",
    "\n",
    "    update_for_q_1_prim = qn_l1_targ(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0)) #targ = target\n",
    "    update_for_q_1_prim = np.squeeze(update_for_q_1_prim, axis=0)\n",
    "    qlabels_l1 = update_for_q_1_prim.copy()\n",
    "    qlabels_l1[np.arange(batch_length), labels_beta1] = np.squeeze(qn_l2_targ(np.expand_dims(s_2_batch, axis=0)).numpy())[np.arange(batch_length),opt_a_2_prim]\n",
    "\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_l1_prim.trainable_variables)\n",
    "            pred_q_1s = qn_l1_prim(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0))\n",
    "\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_1s, qlabels_l1)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "            grads = tape.gradient(loss, qn_l1_prim.trainable_variables)\n",
    "\n",
    "            optimizer_ql1.apply_gradients(zip(grads, qn_l1_prim.trainable_variables))\n",
    "            \n",
    "    s_2_batch = np.array([[v[0], v[2]] for v in batch])\n",
    "    s_3_batch = np.array([[v[0], v[1], v[2], v[3]] for v in batch])\n",
    "\n",
    "    #labels_guess = np.array([v[7] for v in batch])\n",
    "    labels_action_2 = np.array([v[5] for v in batch])\n",
    "\n",
    "    q_3_prim = qn_guess_prim(np.expand_dims(s_3_batch, axis=0))\n",
    "    q_3_prim = np.squeeze(q_3_prim.numpy())\n",
    "\n",
    "    opt_a_3_prim = np.argmax(q_3_prim, axis=1)\n",
    "\n",
    "    update_for_q_2_prim = qn_l2_targ(np.expand_dims(s_2_batch, axis=0))\n",
    "    update_for_q_2_prim = np.squeeze(update_for_q_2_prim, axis=0)\n",
    "    qlabels_l2 = update_for_q_2_prim.copy()\n",
    "    qlabels_l2[np.arange(batch_length), labels_action_2] = np.squeeze(qn_guess_targ(np.expand_dims(s_3_batch, axis=0)).numpy())[np.arange(batch_length), opt_a_3_prim]\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_l2_prim.trainable_variables)\n",
    "            pred_q_2s = qn_l2_prim(np.expand_dims(s_2_batch, axis=0))\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_2s, qlabels_l2)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "            grads = tape.gradient(loss, qn_l2_prim.trainable_variables)\n",
    "            optimizer_ql2.apply_gradients(zip(grads, qn_l2_prim.trainable_variables))\n",
    "\n",
    "\n",
    "    s_3_batch = np.array([[v[0], v[1], v[2], v[3]] for v in batch])\n",
    "    rewards = np.array([v[-1] for v in batch])\n",
    "    labels_guess = np.array([v[7] for v in batch])\n",
    "\n",
    "    update_for_q_3_prim = qn_guess_targ(np.expand_dims(s_3_batch, axis=0))\n",
    "    update_for_q_3_prim = np.squeeze(update_for_q_3_prim, axis=0)\n",
    "    qlabels_l3 = update_for_q_3_prim.copy()\n",
    "    qlabels_l3[np.arange(batch_length), labels_guess] = rewards[np.arange(batch_length)]\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_guess_prim.trainable_variables)\n",
    "            pred_q_3s = qn_guess_prim(np.expand_dims(s_3_batch, axis=0))\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_3s, qlabels_l3)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "            grads = tape.gradient(loss, qn_guess_prim.trainable_variables)\n",
    "            optimizer_ql3.apply_gradients(zip(grads, qn_guess_prim.trainable_variables))\n",
    "            \n",
    "    for t, e in zip(qn_l1_targ.trainable_variables, qn_l1_prim.trainable_variables):\n",
    "        t.assign(t*(1-TAU) + e*TAU)\n",
    "\n",
    "    for t, e in zip(qn_l2_targ.trainable_variables, qn_l2_prim.trainable_variables):\n",
    "        t.assign(t*(1-TAU) + e*TAU)\n",
    "\n",
    "    for t, e in zip(qn_guess_targ.trainable_variables, qn_guess_prim.trainable_variables):\n",
    "        t.assign(t*(1-TAU) + e*TAU)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(beta, alpha, n):\n",
    "    p0 = np.exp(-(beta-alpha)**2)\n",
    "    if n == 0:\n",
    "        return p0\n",
    "    else:\n",
    "        return 1-p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_greedy():\n",
    "    p=0\n",
    "    label1, beta1 = give_first_beta(epsilon=0)\n",
    "    for n1 in [0,1]:\n",
    "        for n2 in [0,1]:\n",
    "            l2, beta2 = give_second_beta([n1, beta1], epsilon=0)\n",
    "            gueslabel, guess_phase = give_guess([n1,n2,beta1,beta2], epsilon=0)\n",
    "            \n",
    "            p+= prob(beta1, guess_phase*np.cos(ats[0])*alpha,n1)*prob(beta2, guess_phase*np.sin(ats[0])*alpha, n2)\n",
    "    return p/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Memory(10**4)\n",
    "optimizer_ql1 = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "optimizer_ql2 = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "optimizer_ql3 = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "alpha = .56\n",
    "states_wasted = 10**2\n",
    "TAU = 0.01\n",
    "\n",
    "cumulative = []\n",
    "def main():\n",
    "    cum_rews=0\n",
    "    for episode in range(states_wasted):\n",
    "        if episode%100 == 0:\n",
    "            print(episode, \" of \", states_wasted)\n",
    "\n",
    "        epsilon = 0.01\n",
    "        phase = np.random.choice([-1,1],1)[0]\n",
    "        labelbeta1, beta1 = give_first_beta(epsilon)\n",
    "        p0 = np.exp(-(beta1-(phase*np.cos(ats[0])*alpha))**2)\n",
    "        outcome1 = np.random.choice([0,1],1,p=[p0,1-p0])[0]\n",
    "        new_state = [outcome1, beta1]\n",
    "        labelbeta2, beta2 = give_second_beta(new_state,epsilon)\n",
    "        p1 = np.exp(-(beta2-(phase*np.sin(ats[0])*alpha))**2)\n",
    "        outcome2 = np.random.choice([0,1],1,p=[p1,1-p1])[0]\n",
    "        new_state = [outcome1, outcome2, beta1, beta2]\n",
    "        label_guess, guess = give_guess(new_state,epsilon)\n",
    "        if guess == phase:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        buffer.add_sample((outcome1, outcome2, beta1, beta2, labelbeta1, labelbeta2, guess, label_guess, reward))\n",
    "        if episode > 10**2:\n",
    "            learn()\n",
    "        cum_rews += reward\n",
    "        cumulative.append(cum_rews)\n",
    "    return cumulative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.5       , 0.33333333, ..., 0.81856371, 0.81858186,\n",
       "       0.8186    ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative/np.arange(1,states_wasted+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7295, 7296, 7297, ..., 9997, 9998, 9999]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(cumulative/np.arange(1,states_wasted+1) > 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f435d7e6240>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cumulative/np.arange(1,states_wasted+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*2*100*2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
