{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, layer=0):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = Dense(2, input_shape=(1,), kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l2 = Dense(2, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l3 = Dense(1, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "        self.loss = tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    def call(self, input):\n",
    "        state, action = input\n",
    "        features = tf.concat([state, action], axis=1)\n",
    "        feat = tf.nn.relu(self.l1(features))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        value = self.l3(feat)\n",
    "        return tf.nn.tanh(value)*2 #Tanh image is [-1,1]\n",
    "\n",
    "    @tf.function\n",
    "    def step_critic(self,labels, states):\n",
    "        #Takes as input [ [ array_of_[states]  ]  , [ array_of_ [actions]  ]  ]\n",
    "        #and labels = np.array(N=len(array_of_[states]=len_array_of_[actions]))\n",
    "        with tf.GradientTape() as tape:\n",
    "            l = self.loss(labels, self.call(states))\n",
    "            l = tf.reduce_mean(l)\n",
    "        g = tape.gradient(l, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(g, self.trainable_variables))\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, layer=0):\n",
    "        super(Actor,self).__init__()\n",
    "        self.l1 = Dense(2, input_shape=(1,), kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l2 = Dense(2, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l3 = Dense(1, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "        self.loss = tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        action = tf.nn.relu(self.l3(feat))\n",
    "        return tf.nn.tanh(action)*2 #Tanh image is [-1,1]\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def step_actor(labels):\n",
    "        #Takes as input np.array(whatever)[...,tf.newaxis]\n",
    "        dummy_states = np.zeros(len(labels))[...,tf.newaxis]\n",
    "        with tf.GradientTape() as tape:\n",
    "            l = loss_mse(labels, pol(dummy_states))\n",
    "            l = tf.reduce_mean(l)\n",
    "        grad = tape.gradient(l,pol.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad, pol.trainable_variables))\n",
    "        return\n",
    "\n",
    "def P(a,beta,n):\n",
    "    p = np.exp(-(a-beta)**2)\n",
    "    if n==0:\n",
    "        return p\n",
    "    else:\n",
    "        return 1-p\n",
    "\n",
    "def give_beta():\n",
    "    input_dummy = np.array([0.])[...,tf.newaxis]\n",
    "    beta = network(input_dummy)\n",
    "    return beta + tf.random.normal(shape=beta.shape, mean=0., stddev = .05, dtype=tf.float64)\n",
    "\n",
    "def make_bet(outcome, beta):\n",
    "    # guess = NN_guess(state)\n",
    "    phases=[-1,1]\n",
    "    guess = np.argmax([P(ph*amplitude, beta, outcome) for ph in phases])\n",
    "    return phases[guess]\n",
    "\n",
    "def obtain_reward(outcome, beta, message):\n",
    "    opinion = make_bet(outcome, beta)\n",
    "    if opinion==message:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def step_learn_actor_ddpg(states_actions):\n",
    "    with tf.GradientTape() as tape:\n",
    "        policy_value = actor(np.zeros(len(states_actions[0]))[...,tf.newaxis])\n",
    "        q_values = critic(states_actions)\n",
    "        product = tf.multiply(policy_value, q_values)\n",
    "        actor_loss = tf.reduce_mean(product) # This is intended to be \\sum_n \\dev Q(s,a) \\def \\mu\n",
    "    g = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor.optimizer.apply_gradients(zip(g,actor.trainable_variables))\n",
    "    return\n",
    "\n",
    "\n",
    "amplitude=.56\n",
    "layers=1\n",
    "buffer_simpler = []\n",
    "critic = Critic()\n",
    "actor = Actor()\n",
    "def body(states_wasted=10):\n",
    "    for experiments in tqdm(range(states_wasted)):\n",
    "        message = np.random.choice([-1,1],1)[0]\n",
    "        # state={\"outcomes\":[], \"betas\":[]}\n",
    "        for layer in range(layers): #in this case one round\n",
    "            # beta_tf = give_beta(actor)\n",
    "            beta_tf = actor(np.array([0.])[...,tf.newaxis])\n",
    "            beta = beta_tf.numpy().flatten()[0]\n",
    "            beta = np.float64(beta)\n",
    "            outcome = np.random.choice([0,1],1,p=[P(message*amplitude, beta, n) for n in [0,1]])[0]\n",
    "            outcome = np.float64(outcome)\n",
    "            # state[\"outcomes\"].append(outcome)\n",
    "            # state[\"betas\"].append(beta)\n",
    "        reward = obtain_reward(outcome,beta, message)\n",
    "        #buffer.add(state, reward)\n",
    "        buffer_simpler.append([beta, outcome, reward])\n",
    "        \n",
    "        q_value_critic = critic([ [ [outcome] ] , [ [beta] ] ])\n",
    "        if len(buffer_simpler)<100:\n",
    "            pass\n",
    "        else:\n",
    "            # td_errors = []\n",
    "            states_actions = [[], []]\n",
    "            labels = []\n",
    "            if experiments%10 ==0:\n",
    "                index_to_sample = np.random.choice(range(len(buffer_simpler)),100)\n",
    "                for index in index_to_sample:\n",
    "                    sample = buffer_simpler[index]\n",
    "                    labels.append(sample[2])\n",
    "                    states_actions[0].append([sample[1]])\n",
    "                    states_actions[1].append([sample[0]]) #notice it's important to have\n",
    "\n",
    "                critic.step_critic(labels, states_actions)\n",
    "                step_learn_actor_ddpg(states_actions)\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
