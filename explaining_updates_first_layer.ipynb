{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import basics\n",
    "import misc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import random\n",
    "\n",
    "basic = basics.Basics(resolution=.25)\n",
    "basic.define_actions()\n",
    "actions = basic.actions\n",
    "ats = misc.make_attenuations(layers=2)\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._samples = []\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self._max_memory:\n",
    "            self._samples.pop(0)\n",
    "    def sample(self, no_samples):\n",
    "        if no_samples > len(self._samples):\n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else:\n",
    "            return random.sample(self._samples, no_samples)\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "\n",
    "\n",
    "\n",
    "cardinality_betas = len(basic.actions[0])\n",
    "\n",
    "class QN_l1(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_l1,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(0,), kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "        self.l2 = Dense(35, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "\n",
    "        # self.l21 = Dense(90, kernel_initializer='random_uniform',\n",
    "        #         bias_initializer='random_uniform')\n",
    "        self.l3 = Dense(cardinality_betas, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "class QN_l2(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_l2,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(1,2), kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "        self.l2 = Dense(35, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "\n",
    "        # self.l21 = Dense(90, kernel_initializer='random_uniform',\n",
    "        #         bias_initializer='random_uniform')\n",
    "        self.l3 = Dense(cardinality_betas, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "class QN_guess(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_guess,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(1,4), kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l2 = Dense(35, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "        # self.l21 = Dense(90, kernel_initializer='random_uniform',\n",
    "        #         bias_initializer='random_uniform')\n",
    "        self.l3 = Dense(2, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "\n",
    "#### define the networks #####\n",
    "\n",
    "qn_l1_prim = QN_l1()\n",
    "qn_l1_targ = QN_l1()\n",
    "\n",
    "qn_l2_prim = QN_l2()\n",
    "qn_l2_targ = QN_l2()\n",
    "\n",
    "qn_guess_prim = QN_guess()\n",
    "qn_guess_targ = QN_guess()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def give_first_beta(epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        label = np.random.choice(np.arange(len(basic.actions[0])))\n",
    "        return label, basic.actions[0][label]\n",
    "    else:\n",
    "        input = np.expand_dims(np.array([]), axis=0)\n",
    "        q1s = qn_l1_prim(input)\n",
    "        q1s = q1s.numpy()\n",
    "        label = np.argmax(q1s)\n",
    "        beta1 = basic.actions[0][label]\n",
    "        return label, beta1\n",
    "\n",
    "def give_second_beta(new_state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        label = np.random.choice(np.arange(len(basic.actions[1])))\n",
    "        return label, basic.actions[1][label]\n",
    "    else:\n",
    "        input = np.expand_dims(np.array(new_state), axis=0)\n",
    "        q2s = qn_l2_prim(input)\n",
    "        q2s = q2s.numpy()\n",
    "        label = np.argmax(q2s)\n",
    "        beta2 = basic.actions[1][label]\n",
    "        return label, beta2\n",
    "\n",
    "\n",
    "def give_guess(new_state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        guess = np.random.choice(basic.possible_phases,1)[0]\n",
    "        return guess\n",
    "    else:\n",
    "        input = np.expand_dims(np.array(new_state), axis=0)\n",
    "        qguess = qn_guess_prim(input)\n",
    "        guess = qguess.numpy()\n",
    "        label = np.argmax(guess)\n",
    "        guess = basic.possible_phases[label]\n",
    "        return guess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1103 12:20:49.810138 140411243505472 base_layer.py:1865] Layer qn_l1_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1103 12:20:49.838775 140411243505472 base_layer.py:1865] Layer qn_l2_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1103 12:20:49.860404 140411243505472 base_layer.py:1865] Layer qn_guess_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "buffer = Memory(10**4)\n",
    "\n",
    "alpha = .56\n",
    "states_wasted = 10**3\n",
    "\n",
    "def main():\n",
    "    for episode in range(states_wasted):\n",
    "        epsilon = np.exp(-0.001*episode)\n",
    "        phase = np.random.choice([-1,1],1)[0]\n",
    "        labelbeta1, beta1 = give_first_beta(epsilon)\n",
    "        p0 = np.exp(-(beta1-(phase*np.cos(ats[0])*alpha))**2)\n",
    "        outcome1 = np.random.choice([0,1],1,p=[p0,1-p0])[0]\n",
    "        new_state = [outcome1, beta1]\n",
    "        labelbeta2, beta2 = give_second_beta(new_state,epsilon)\n",
    "        p1 = np.exp(-(beta2-(phase*np.sin(ats[0])*alpha))**2)\n",
    "        outcome2 = np.random.choice([0,1],1,p=[p1,1-p1])[0]\n",
    "        new_state = [outcome1, outcome2, beta1, beta2]\n",
    "        guess = give_guess(new_state,epsilon)\n",
    "        if guess == phase:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        buffer.add_sample((outcome1, outcome2, beta1, beta2, labelbeta1, labelbeta2, guess, reward))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0.0, -0.5, 4, 1, 1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 1, -0.5, 1.0, 2, 4, 1, 1),\n",
       " (0, 0, -0.25, 0.5, 3, 3, -1, 0),\n",
       " (0, 0, -0.25, 1.0, 3, 4, 1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 0, -0.5, 0.0, 2, 2, -1, 1),\n",
       " (0, 0, 0.0, -0.5, 4, 1, 1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 1, -0.5, -1.0, 2, 0, -1, 0),\n",
       " (1, 0, -0.75, 0.0, 1, 2, 1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (1, 0, -0.25, -0.5, 3, 1, -1, 0),\n",
       " (0, 1, -0.25, 0.0, 3, 2, -1, 1),\n",
       " (1, 0, -0.75, 0.5, 1, 3, 1, 1),\n",
       " (0, 1, -0.5, 1.0, 2, 4, 1, 0),\n",
       " (0, 0, 0.0, -0.5, 4, 1, -1, 1),\n",
       " (1, 1, -1.0, 0.5, 0, 3, -1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 1, -0.75, -1.0, 1, 0, 1, 1),\n",
       " (0, 0, -0.5, -1.0, 2, 0, -1, 1),\n",
       " (1, 0, -0.25, 0.5, 3, 3, -1, 0),\n",
       " (1, 1, 0.0, 1.0, 4, 4, -1, 1),\n",
       " (0, 0, -0.25, 0.5, 3, 3, -1, 1),\n",
       " (1, 0, -1.0, 0.0, 0, 2, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 0, 0.0, 0.5, 4, 3, -1, 1),\n",
       " (0, 1, -1.0, -1.0, 0, 0, 1, 1),\n",
       " (0, 0, -1.0, -1.0, 0, 0, 1, 0),\n",
       " (1, 0, -0.75, 0.5, 1, 3, 1, 1),\n",
       " (1, 0, -1.0, 0.5, 0, 3, -1, 0),\n",
       " (0, 1, -0.75, 0.5, 1, 3, -1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 0, -0.5, -0.5, 2, 1, -1, 1),\n",
       " (0, 0, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (1, 0, -0.5, -0.5, 2, 1, -1, 0),\n",
       " (0, 0, -0.75, 0.5, 1, 3, -1, 0),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 1, 0.0, -1.0, 4, 0, 1, 1),\n",
       " (1, 1, -0.5, 0.0, 2, 2, -1, 0),\n",
       " (0, 0, 0.0, 1.0, 4, 4, 1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (0, 1, 0.0, -1.0, 4, 0, -1, 0),\n",
       " (1, 0, -1.0, 0.5, 0, 3, -1, 0),\n",
       " (1, 0, -0.25, 0.5, 3, 3, -1, 0),\n",
       " (1, 0, -0.5, -0.5, 2, 1, -1, 0),\n",
       " (1, 0, -1.0, 0.5, 0, 3, -1, 0),\n",
       " (0, 1, -0.5, 1.0, 2, 4, 1, 0),\n",
       " (0, 1, -0.25, -0.5, 3, 1, -1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 1, -0.5, 1.0, 2, 4, -1, 1),\n",
       " (0, 0, -0.25, 0.0, 3, 2, -1, 1),\n",
       " (0, 0, -0.25, 0.0, 3, 2, -1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 1),\n",
       " (0, 1, -0.5, 1.0, 2, 4, -1, 1),\n",
       " (0, 1, -0.5, -1.0, 2, 0, -1, 1),\n",
       " (1, 1, 0.0, 0.0, 4, 2, -1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 1, -0.5, 1.0, 2, 4, 1, 0),\n",
       " (1, 0, -0.75, -1.0, 1, 0, -1, 0),\n",
       " (0, 1, -0.75, 0.5, 1, 3, -1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 0, -0.5, 1.0, 2, 4, 1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 0, -0.5, 0.0, 2, 2, 1, 1),\n",
       " (0, 0, -0.5, -0.5, 2, 1, 1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 1),\n",
       " (1, 1, -0.5, -0.5, 2, 1, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 0, -0.5, 0.5, 2, 3, 1, 1),\n",
       " (0, 0, -0.25, 0.5, 3, 3, -1, 1),\n",
       " (0, 1, -0.25, 0.5, 3, 3, -1, 1),\n",
       " (0, 0, -0.25, -0.5, 3, 1, -1, 0),\n",
       " (0, 0, -0.5, -0.5, 2, 1, 1, 0),\n",
       " (0, 0, -0.5, -0.5, 2, 1, -1, 1),\n",
       " (1, 0, 0.0, 1.0, 4, 4, -1, 0),\n",
       " (1, 0, -0.25, 1.0, 3, 4, -1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (0, 1, -0.5, -1.0, 2, 0, 1, 1),\n",
       " (0, 0, -0.5, 0.0, 2, 2, -1, 1),\n",
       " (0, 1, -0.75, 0.5, 1, 3, 1, 0),\n",
       " (0, 1, -1.0, 0.5, 0, 3, -1, 1),\n",
       " (1, 0, -0.25, 0.5, 3, 3, -1, 0),\n",
       " (1, 0, -0.25, 0.5, 3, 3, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 0, -0.5, 0.0, 2, 2, -1, 1),\n",
       " (1, 1, -1.0, 1.0, 0, 4, -1, 0),\n",
       " (0, 0, 0.0, 0.5, 4, 3, 1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 0, -0.5, 0.0, 2, 2, -1, 0),\n",
       " (1, 0, -0.75, 1.0, 1, 4, -1, 0),\n",
       " (1, 1, -0.5, 1.0, 2, 4, -1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 0, -0.5, 0.0, 2, 2, -1, 0),\n",
       " (0, 1, -0.25, 1.0, 3, 4, 1, 1),\n",
       " (0, 0, -0.25, 0.5, 3, 3, 1, 1),\n",
       " (0, 0, -1.0, -1.0, 0, 0, 1, 0),\n",
       " (1, 0, 0.0, 1.0, 4, 4, -1, 0),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 0, -1.0, 0.5, 0, 3, -1, 1),\n",
       " (1, 0, -0.75, 0.5, 1, 3, 1, 1),\n",
       " (1, 0, -1.0, 0.5, 0, 3, -1, 0),\n",
       " (0, 1, -0.25, 0.5, 3, 3, -1, 1),\n",
       " (0, 0, -0.5, 0.0, 2, 2, 1, 0),\n",
       " (0, 0, -1.0, 0.0, 0, 2, -1, 1),\n",
       " (0, 1, -0.5, -0.5, 2, 1, 1, 1),\n",
       " (0, 0, -0.5, 0.0, 2, 2, 1, 0),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (1, 0, -0.5, 0.0, 2, 2, 1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 1, -0.75, 0.5, 1, 3, -1, 1),\n",
       " (0, 1, 0.0, 0.0, 4, 2, 1, 0),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (1, 0, -0.5, 1.0, 2, 4, 1, 1),\n",
       " (0, 1, -1.0, 0.5, 0, 3, 1, 0),\n",
       " (1, 0, -0.75, -1.0, 1, 0, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (1, 1, -0.5, -0.5, 2, 1, -1, 0),\n",
       " (1, 1, -0.5, -1.0, 2, 0, -1, 0),\n",
       " (0, 0, -0.5, 0.0, 2, 2, -1, 1),\n",
       " (1, 1, -0.5, 1.0, 2, 4, 1, 1),\n",
       " (1, 0, -0.5, 1.0, 2, 4, -1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 1, -1.0, -1.0, 0, 0, -1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, -1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 0, -0.5, 1.0, 2, 4, 1, 1),\n",
       " (0, 0, -0.25, -0.5, 3, 1, -1, 1),\n",
       " (0, 1, -0.5, -1.0, 2, 0, -1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 1, -0.25, -0.5, 3, 1, -1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 1, -0.5, 0.0, 2, 2, -1, 1),\n",
       " (0, 1, -1.0, 0.5, 0, 3, -1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 1),\n",
       " (0, 1, -0.5, 1.0, 2, 4, 1, 0),\n",
       " (1, 0, -1.0, 0.5, 0, 3, 1, 1),\n",
       " (1, 1, -1.0, 1.0, 0, 4, -1, 0),\n",
       " (1, 0, -0.75, 1.0, 1, 4, 1, 1),\n",
       " (0, 0, -0.25, 0.5, 3, 3, 1, 1),\n",
       " (1, 0, -1.0, 0.5, 0, 3, -1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (1, 1, -1.0, 1.0, 0, 4, -1, 1),\n",
       " (1, 0, -0.25, 1.0, 3, 4, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 0, -1.0, -0.5, 0, 1, -1, 1),\n",
       " (0, 1, -0.25, 0.5, 3, 3, 1, 0),\n",
       " (0, 1, -0.75, 0.5, 1, 3, -1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (1, 0, -0.5, -0.5, 2, 1, 1, 1),\n",
       " (1, 0, -0.5, 1.0, 2, 4, -1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 0, -1.0, -1.0, 0, 0, -1, 1),\n",
       " (1, 1, -0.75, -1.0, 1, 0, 1, 1),\n",
       " (1, 0, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 1, -0.75, 1.0, 1, 4, -1, 1),\n",
       " (1, 1, -0.5, -0.5, 2, 1, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 1),\n",
       " (0, 1, -0.5, 0.0, 2, 2, 1, 1),\n",
       " (1, 0, 0.0, 0.5, 4, 3, 1, 0),\n",
       " (1, 0, -0.75, 0.5, 1, 3, -1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 0, -0.75, 0.5, 1, 3, 1, 1),\n",
       " (0, 0, -0.25, -0.5, 3, 1, 1, 1),\n",
       " (0, 1, 0.0, 0.0, 4, 2, -1, 1),\n",
       " (0, 0, -0.75, -0.5, 1, 1, -1, 0),\n",
       " (1, 1, -0.75, -0.5, 1, 1, 1, 1),\n",
       " (1, 0, -0.5, 1.0, 2, 4, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 0, 0.0, -0.5, 4, 1, -1, 0),\n",
       " (1, 0, -0.5, 0.0, 2, 2, 1, 1),\n",
       " (1, 0, 0.0, 0.0, 4, 2, -1, 1),\n",
       " (1, 0, -0.75, 0.5, 1, 3, -1, 0),\n",
       " (0, 1, -0.25, 0.5, 3, 3, -1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 0, -0.25, 0.0, 3, 2, 1, 0),\n",
       " (0, 0, -0.5, -0.5, 2, 1, -1, 1),\n",
       " (1, 1, -0.5, -1.0, 2, 0, -1, 0),\n",
       " (1, 0, -0.5, 1.0, 2, 4, 1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 1, -0.25, 0.5, 3, 3, -1, 1),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (1, 1, -1.0, -0.5, 0, 1, 1, 1),\n",
       " (0, 1, -0.25, -1.0, 3, 0, 1, 1),\n",
       " (0, 0, -0.25, 0.0, 3, 2, 1, 0),\n",
       " (1, 1, -0.25, -1.0, 3, 0, 1, 1),\n",
       " (1, 0, -1.0, 0.5, 0, 3, -1, 0),\n",
       " (1, 1, -0.5, -1.0, 2, 0, -1, 0),\n",
       " (0, 0, -0.5, -0.5, 2, 1, -1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (1, 0, -1.0, 0.5, 0, 3, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (1, 0, -0.5, 0.0, 2, 2, -1, 0),\n",
       " (0, 0, -0.5, -0.5, 2, 1, -1, 1),\n",
       " (1, 1, -1.0, 0.0, 0, 2, -1, 1),\n",
       " (0, 0, -0.5, -0.5, 2, 1, -1, 1),\n",
       " (0, 1, -0.5, -1.0, 2, 0, -1, 0),\n",
       " (0, 1, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (0, 0, 0.0, 0.5, 4, 3, -1, 0),\n",
       " (0, 0, 0.0, 0.5, 4, 3, 1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 0, -1.0, 0.5, 0, 3, -1, 1),\n",
       " (0, 0, -0.25, 0.5, 3, 3, -1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 0, -0.5, -0.5, 2, 1, -1, 1),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 1, -0.5, 1.0, 2, 4, -1, 0),\n",
       " (1, 0, -0.25, 0.5, 3, 3, -1, 0),\n",
       " (0, 0, 0.0, 1.0, 4, 4, 1, 1),\n",
       " (0, 1, -0.25, 1.0, 3, 4, -1, 1),\n",
       " (0, 1, -0.75, 0.5, 1, 3, -1, 1),\n",
       " (1, 0, 0.0, 0.5, 4, 3, -1, 0),\n",
       " (0, 1, -0.75, 0.5, 1, 3, 1, 0),\n",
       " (0, 1, -0.5, 1.0, 2, 4, -1, 1),\n",
       " (1, 1, -0.75, 1.0, 1, 4, -1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 1, -0.5, 1.0, 2, 4, -1, 1),\n",
       " (0, 1, -0.75, 1.0, 1, 4, -1, 1),\n",
       " (0, 0, -0.5, -0.5, 2, 1, 1, 1),\n",
       " (0, 0, 0.0, 1.0, 4, 4, -1, 0),\n",
       " (0, 1, -0.25, 0.5, 3, 3, -1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, 1, 1),\n",
       " (0, 0, -0.25, -0.5, 3, 1, -1, 0),\n",
       " (1, 0, -0.75, -0.5, 1, 1, -1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (0, 0, 0.0, -0.5, 4, 1, -1, 1),\n",
       " (1, 1, -0.5, -0.5, 2, 1, 1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (1, 0, -0.5, 0.0, 2, 2, -1, 0),\n",
       " (0, 1, -0.5, 1.0, 2, 4, 1, 0),\n",
       " (1, 1, -0.75, -1.0, 1, 0, -1, 0),\n",
       " (0, 1, -0.5, -1.0, 2, 0, -1, 1),\n",
       " (0, 1, -0.25, 0.5, 3, 3, -1, 1),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (0, 0, -0.25, 0.5, 3, 3, -1, 0),\n",
       " (1, 0, -0.75, 0.5, 1, 3, -1, 0),\n",
       " (1, 0, -0.25, 1.0, 3, 4, -1, 0),\n",
       " (1, 0, -0.25, -1.0, 3, 0, 1, 1),\n",
       " (1, 0, -0.5, 0.5, 2, 3, 1, 1),\n",
       " (0, 1, -0.5, -1.0, 2, 0, 1, 1),\n",
       " (0, 1, -1.0, 1.0, 0, 4, 1, 0),\n",
       " (0, 1, 0.0, 1.0, 4, 4, 1, 0),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 1, -0.25, 0.5, 3, 3, -1, 1),\n",
       " (1, 1, -0.5, -0.5, 2, 1, 1, 1),\n",
       " (1, 0, -1.0, 0.5, 0, 3, -1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, -1, 1),\n",
       " (1, 0, -0.5, 1.0, 2, 4, -1, 0),\n",
       " (1, 0, -0.5, 1.0, 2, 4, -1, 0),\n",
       " (0, 1, -0.5, 1.0, 2, 4, -1, 1),\n",
       " (1, 0, -0.75, 0.5, 1, 3, 1, 1),\n",
       " (0, 1, -0.5, -1.0, 2, 0, -1, 0),\n",
       " (1, 0, -1.0, 0.5, 0, 3, 1, 0),\n",
       " (1, 0, 0.0, -0.5, 4, 1, -1, 0),\n",
       " (0, 1, -0.25, 1.0, 3, 4, 1, 0),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (1, 1, -0.5, -1.0, 2, 0, -1, 0),\n",
       " (1, 0, -0.75, 0.0, 1, 2, -1, 0),\n",
       " (0, 1, 0.0, 0.0, 4, 2, -1, 1),\n",
       " (0, 0, -0.25, 0.0, 3, 2, -1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, 1, 1),\n",
       " (0, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (1, 0, -0.75, 0.5, 1, 3, 1, 1),\n",
       " (0, 0, -0.25, -0.5, 3, 1, 1, 0),\n",
       " (0, 0, 0.0, -0.5, 4, 1, 1, 0),\n",
       " (1, 0, -0.5, 1.0, 2, 4, 1, 1),\n",
       " (0, 0, -0.5, 0.0, 2, 2, 1, 0),\n",
       " (0, 1, -0.5, 0.5, 2, 3, 1, 0),\n",
       " (0, 1, -0.5, 1.0, 2, 4, 1, 0),\n",
       " (1, 0, -0.5, 0.5, 2, 3, -1, 0),\n",
       " (0, 0, -0.25, -0.5, 3, 1, 1, 0),\n",
       " (0, 0, -0.75, -0.5, 1, 1, -1, 1),\n",
       " (0, 1, -0.5, -1.0, 2, 0, 1, 1),\n",
       " (0, 0, -0.5, 0.0, 2, 2, -1, 1),\n",
       " (0, 0, -0.25, -0.5, 3, 1, 1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, -1, 0),\n",
       " (0, 0, 0.0, 0.0, 4, 2, 1, 1),\n",
       " (0, 1, -0.5, -1.0, 2, 0, -1, 0),\n",
       " (0, 0, -0.75, -0.5, 1, 1, 1, 0),\n",
       " (1, 0, 0.0, 1.0, 4, 4, 1, 1),\n",
       " (1, 1, -0.75, 0.5, 1, 3, -1, 1),\n",
       " (1, 0, 0.0, 0.5, 4, 3, -1, 0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = buffer.sample(300)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates: $$Q_{l=1}[s = \\phi, a = \\beta_1] <- Q^{target} [ n_1, \\beta_1; \\tilde{\\beta}_2] \\\\ \\text{For the $\\beta_1$ selected, otherwise no update (not a detail! see later that the update for those betas is exactly the same, so the loss is only the\n",
    "\n",
    "line above for the single beta selected at each episode in the batch...}$$ where $$ \\tilde{\\beta}_2 = \\underset{\\beta_2}{\\text{argmax}} Q_{l=2}[ s=(n_1, \\beta_1), a = \\beta_2 ] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buffer.add_sample((outcome1, outcome2, beta1, beta2, labelbeta1, labelbeta2, guess, reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_l2 = np.array([[ v[0], v[2]] for v in batch ] )\n",
    "labels_beta1 = np.array([v[4] for v in batch])\n",
    "\n",
    "layer1p1_normal = qn_l2_prim(np.expand_dims(states_l2, axis=0))\n",
    "layer1p1_normal = np.squeeze(layer1p1_normal.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33491045, -0.07160825,  0.76262116,  0.65361667, -0.07509165],\n",
       "       [ 0.2502162 , -0.4565762 , -0.02420005,  0.6332496 , -0.5558549 ],\n",
       "       [ 0.2502162 , -0.4565762 , -0.02420005,  0.6332496 , -0.5558549 ],\n",
       "       ...,\n",
       "       [ 0.12944512, -0.40295878,  0.83297133,  1.3347999 ,  0.45993352],\n",
       "       [ 0.30730686, -0.66711074, -0.01373112,  1.2221037 , -0.13811909],\n",
       "       [ 0.12944512, -0.40295878,  0.83297133,  1.3347999 ,  0.45993352]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1p1_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1p1_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardinality_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls=[]\n",
    "for i in range(300):\n",
    "    lbls.append(np.argmax(layer1p1_normal[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3,\n",
       "       2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2,\n",
       "       2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimals_beta2_prim_labels = np.argmax(layer1p1_normal,axis=1)\n",
    "optimals_beta2_prim_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimals_beta2_prim_labels - np.array(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1103 12:23:57.597498 140411243505472 base_layer.py:1865] Layer qn_l1_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_qlp1 = qn_l1_targ(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0))\n",
    "target_qlp1 = np.squeeze(target_qlp1, axis=0)\n",
    "targs = target_qlp1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i check that only update the ones that experience ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "targs[np.arange(300), labels_beta1] = np.squeeze(qn_l2_targ(np.expand_dims(states_l2, axis=0)).numpy())[np.arange(300),optimals_beta2_prim_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_qlp1 = qn_l1_targ(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0))\n",
    "target_qlp1 = np.squeeze(target_qlp1, axis=0)\n",
    "dif = targs - target_qlp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.        0.        0.        0.8028864 0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[-0.28091812  0.          0.          0.          0.        ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.16086757 0.         0.         0.         0.        ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        1.1797621 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.         0.         0.         0.41202188 0.        ]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.        0.        0.6818714]\n",
      "[0.        0.        0.7985648 0.        0.       ]\n",
      "[0.        1.4658215 0.        0.        0.       ]\n",
      "[0.       0.       0.       0.       1.242022]\n",
      "[0.        1.8967357 0.        0.        0.       ]\n",
      "[0.       0.       0.       0.       1.242022]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dif)):\n",
    "    print(dif[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.18601634>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_sum =tf.keras.losses.MSE(targs, target_qlp1)\n",
    "tf.reduce_mean(loss_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ql1 = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(qn_l1_prim.trainable_variables)\n",
    "        \n",
    "        pred_prim = qn_l1_prim(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0))\n",
    "        \n",
    "        loss_sum =tf.keras.losses.MSE(pred_prim, targs)\n",
    "        loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "        grads = tape.gradient(loss, qn_l1_prim.trainable_variables)\n",
    "        optimizer_ql1.apply_gradients(zip(grads, qn_l1_prim.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
