{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1103 15:30:06.593185 139929372481344 base_layer.py:1865] Layer qn_l1_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1103 15:30:06.607066 139929372481344 base_layer.py:1865] Layer qn_l2_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1103 15:30:06.626836 139929372481344 base_layer.py:1865] Layer qn_guess_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1103 15:30:06.691062 139929372481344 base_layer.py:1865] Layer qn_l1_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1103 15:30:06.712627 139929372481344 base_layer.py:1865] Layer qn_l2_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1103 15:30:06.787737 139929372481344 base_layer.py:1865] Layer qn_guess_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  of  10000\n",
      "100  of  10000\n",
      "200  of  10000\n",
      "300  of  10000\n",
      "400  of  10000\n",
      "500  of  10000\n",
      "600  of  10000\n",
      "700  of  10000\n",
      "800  of  10000\n",
      "900  of  10000\n",
      "1000  of  10000\n",
      "1100  of  10000\n",
      "1200  of  10000\n",
      "1300  of  10000\n",
      "1400  of  10000\n",
      "1500  of  10000\n",
      "1600  of  10000\n",
      "1700  of  10000\n",
      "1800  of  10000\n",
      "1900  of  10000\n",
      "2000  of  10000\n",
      "2100  of  10000\n",
      "2200  of  10000\n",
      "2300  of  10000\n",
      "2400  of  10000\n",
      "2500  of  10000\n",
      "2600  of  10000\n",
      "2700  of  10000\n",
      "2800  of  10000\n",
      "2900  of  10000\n",
      "3000  of  10000\n",
      "3100  of  10000\n",
      "3200  of  10000\n",
      "3300  of  10000\n",
      "3400  of  10000\n",
      "3500  of  10000\n",
      "3600  of  10000\n",
      "3700  of  10000\n",
      "3800  of  10000\n",
      "3900  of  10000\n",
      "4000  of  10000\n",
      "4100  of  10000\n",
      "4200  of  10000\n",
      "4300  of  10000\n",
      "4400  of  10000\n",
      "4500  of  10000\n",
      "4600  of  10000\n",
      "4700  of  10000\n",
      "4800  of  10000\n",
      "4900  of  10000\n",
      "5000  of  10000\n",
      "5100  of  10000\n",
      "5200  of  10000\n",
      "5300  of  10000\n",
      "5400  of  10000\n",
      "5500  of  10000\n",
      "5600  of  10000\n",
      "5700  of  10000\n",
      "5800  of  10000\n",
      "5900  of  10000\n",
      "6000  of  10000\n",
      "6100  of  10000\n",
      "6200  of  10000\n",
      "6300  of  10000\n",
      "6400  of  10000\n",
      "6500  of  10000\n",
      "6600  of  10000\n",
      "6700  of  10000\n",
      "6800  of  10000\n",
      "6900  of  10000\n",
      "7000  of  10000\n",
      "7100  of  10000\n",
      "7200  of  10000\n",
      "7300  of  10000\n",
      "7400  of  10000\n",
      "7500  of  10000\n",
      "7600  of  10000\n",
      "7700  of  10000\n",
      "7800  of  10000\n",
      "7900  of  10000\n",
      "8000  of  10000\n",
      "8100  of  10000\n",
      "8200  of  10000\n",
      "8300  of  10000\n",
      "8400  of  10000\n",
      "8500  of  10000\n",
      "8600  of  10000\n",
      "8700  of  10000\n",
      "8800  of  10000\n",
      "8900  of  10000\n",
      "9000  of  10000\n",
      "9100  of  10000\n",
      "9200  of  10000\n",
      "9300  of  10000\n",
      "9400  of  10000\n",
      "9500  of  10000\n",
      "9600  of  10000\n",
      "9700  of  10000\n",
      "9800  of  10000\n",
      "9900  of  10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 39,\n",
       " 39,\n",
       " 40,\n",
       " 40,\n",
       " 41,\n",
       " 41,\n",
       " 41,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 43,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 47,\n",
       " 47,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 58,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 60,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 65,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 68,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 75,\n",
       " 75,\n",
       " 75,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 81,\n",
       " 82,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 86,\n",
       " 86,\n",
       " 87,\n",
       " 87,\n",
       " 88,\n",
       " 88,\n",
       " 89,\n",
       " 89,\n",
       " 90,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 92,\n",
       " 93,\n",
       " 93,\n",
       " 94,\n",
       " 94,\n",
       " 94,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 96,\n",
       " 96,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 110,\n",
       " 110,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 113,\n",
       " 114,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 131,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 136,\n",
       " 137,\n",
       " 137,\n",
       " 138,\n",
       " 138,\n",
       " 139,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 151,\n",
       " 152,\n",
       " 152,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 175,\n",
       " 176,\n",
       " 176,\n",
       " 176,\n",
       " 177,\n",
       " 177,\n",
       " 178,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 181,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 185,\n",
       " 186,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 196,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 199,\n",
       " 199,\n",
       " 199,\n",
       " 199,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 201,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 211,\n",
       " 212,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 221,\n",
       " 222,\n",
       " 222,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 227,\n",
       " 227,\n",
       " 227,\n",
       " 228,\n",
       " 228,\n",
       " 229,\n",
       " 229,\n",
       " 230,\n",
       " 230,\n",
       " 230,\n",
       " 230,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 233,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 241,\n",
       " 242,\n",
       " 242,\n",
       " 243,\n",
       " 243,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 245,\n",
       " 245,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 247,\n",
       " 248,\n",
       " 248,\n",
       " 249,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 253,\n",
       " 253,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 256,\n",
       " 256,\n",
       " 256,\n",
       " 256,\n",
       " 256,\n",
       " 257,\n",
       " 257,\n",
       " 258,\n",
       " 258,\n",
       " 258,\n",
       " 259,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 267,\n",
       " 267,\n",
       " 267,\n",
       " 268,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 272,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 274,\n",
       " 274,\n",
       " 275,\n",
       " 275,\n",
       " 276,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 278,\n",
       " 279,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 281,\n",
       " 282,\n",
       " 282,\n",
       " 282,\n",
       " 282,\n",
       " 282,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 291,\n",
       " 291,\n",
       " 292,\n",
       " 292,\n",
       " 292,\n",
       " 293,\n",
       " 293,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 296,\n",
       " 297,\n",
       " 297,\n",
       " 298,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 300,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 303,\n",
       " 303,\n",
       " 304,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 308,\n",
       " 308,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 313,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 315,\n",
       " 316,\n",
       " 316,\n",
       " 317,\n",
       " 317,\n",
       " 317,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 319,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 321,\n",
       " 321,\n",
       " 321,\n",
       " 322,\n",
       " 322,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 324,\n",
       " 324,\n",
       " 324,\n",
       " 324,\n",
       " 324,\n",
       " 324,\n",
       " 324,\n",
       " 325,\n",
       " 325,\n",
       " 325,\n",
       " 326,\n",
       " 326,\n",
       " 326,\n",
       " 326,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 328,\n",
       " 329,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 341,\n",
       " 342,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 346,\n",
       " 347,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 354,\n",
       " 354,\n",
       " 354,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 360,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 364,\n",
       " 365,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 371,\n",
       " 372,\n",
       " 372,\n",
       " 372,\n",
       " 373,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 375,\n",
       " 376,\n",
       " 376,\n",
       " 377,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 379,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 389,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 398,\n",
       " 399,\n",
       " 399,\n",
       " 399,\n",
       " 399,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 420,\n",
       " 420,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 444,\n",
       " 445,\n",
       " 445,\n",
       " 446,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 450,\n",
       " 451,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 467,\n",
       " 467,\n",
       " 468,\n",
       " 468,\n",
       " 468,\n",
       " 468,\n",
       " 469,\n",
       " 469,\n",
       " 470,\n",
       " 470,\n",
       " 470,\n",
       " 471,\n",
       " 471,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 480,\n",
       " 480,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 486,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 503,\n",
       " 503,\n",
       " 503,\n",
       " 503,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 509,\n",
       " 509,\n",
       " 509,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 513,\n",
       " 514,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 521,\n",
       " 521,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 523,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 527,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 533,\n",
       " 534,\n",
       " 534,\n",
       " 534,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 536,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 539,\n",
       " 540,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 546,\n",
       " 546,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 551,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 556,\n",
       " 557,\n",
       " 557,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 559,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 566,\n",
       " 566,\n",
       " 566,\n",
       " 566,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import basics\n",
    "import misc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import random\n",
    "\n",
    "basic = basics.Basics(resolution=.01)\n",
    "basic.define_actions()\n",
    "actions = basic.actions\n",
    "ats = misc.make_attenuations(layers=2)\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._samples = []\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self._max_memory:\n",
    "            self._samples.pop(0)\n",
    "    def sample(self, no_samples):\n",
    "        if no_samples > len(self._samples):\n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else:\n",
    "            return random.sample(self._samples, no_samples)\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "\n",
    "\n",
    "\n",
    "cardinality_betas = len(basic.actions[0])\n",
    "\n",
    "class QN_l1(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_l1,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(0,), kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "        self.l2 = Dense(35, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "\n",
    "        # self.l21 = Dense(90, kernel_initializer='random_uniform',\n",
    "        #         bias_initializer='random_uniform')\n",
    "        self.l3 = Dense(cardinality_betas, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "class QN_l2(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_l2,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(1,2), kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "        self.l2 = Dense(35, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "\n",
    "        # self.l21 = Dense(90, kernel_initializer='random_uniform',\n",
    "        #         bias_initializer='random_uniform')\n",
    "        self.l3 = Dense(cardinality_betas, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "class QN_guess(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_guess,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(1,4), kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l2 = Dense(35, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "        # self.l21 = Dense(90, kernel_initializer='random_uniform',\n",
    "        #         bias_initializer='random_uniform')\n",
    "        self.l3 = Dense(2, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "\n",
    "#### define the networks #####\n",
    "\n",
    "qn_l1_prim = QN_l1()\n",
    "qn_l1_targ = QN_l1()\n",
    "\n",
    "qn_l2_prim = QN_l2()\n",
    "qn_l2_targ = QN_l2()\n",
    "\n",
    "qn_guess_prim = QN_guess()\n",
    "qn_guess_targ = QN_guess()\n",
    "\n",
    "\n",
    "\n",
    "def give_first_beta(epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        label = np.random.choice(np.arange(len(basic.actions[0])))\n",
    "        return label, basic.actions[0][label]\n",
    "    else:\n",
    "        input = np.expand_dims(np.array([]), axis=0)\n",
    "        q1s = qn_l1_prim(input)\n",
    "        q1s = q1s.numpy()\n",
    "        label = np.argmax(q1s)\n",
    "        beta1 = basic.actions[0][label]\n",
    "        return label, beta1\n",
    "\n",
    "def give_second_beta(new_state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        label = np.random.choice(np.arange(len(basic.actions[1])))\n",
    "        return label, basic.actions[1][label]\n",
    "    else:\n",
    "        input = np.expand_dims(np.array(new_state), axis=0)\n",
    "        q2s = qn_l2_prim(input)\n",
    "        q2s = q2s.numpy()\n",
    "        label = np.argmax(q2s)\n",
    "        beta2 = basic.actions[1][label]\n",
    "        return label, beta2\n",
    "\n",
    "\n",
    "def give_guess(new_state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        guess = np.random.choice(basic.possible_phases,1)[0]\n",
    "        return int((guess+1)/2), guess\n",
    "    else:\n",
    "        input = np.expand_dims(np.array(new_state), axis=0)\n",
    "        qguess = qn_guess_prim(input)\n",
    "        guess = qguess.numpy()\n",
    "        label = np.argmax(guess)\n",
    "        guess = basic.possible_phases[label]\n",
    "        return int((guess+1)/2), guess\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def learn():\n",
    "    batch_length=32\n",
    "    batch = buffer.sample(batch_length)\n",
    "\n",
    "    s_2_batch = np.array([[ v[0], v[2]] for v in batch ] )\n",
    "    labels_beta1 = np.array([v[4] for v in batch])\n",
    "\n",
    "    q_2_prim = qn_l2_prim(np.expand_dims(s_2_batch, axis=0))\n",
    "    q_2_prim = np.squeeze(q_2_prim.numpy())\n",
    "\n",
    "    opt_a_2_prim = np.argmax(q_2_prim,axis=1)\n",
    "\n",
    "    update_for_q_1_prim = qn_l1_targ(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0)) #targ = target\n",
    "    update_for_q_1_prim = np.squeeze(update_for_q_1_prim, axis=0)\n",
    "    qlabels_l1 = update_for_q_1_prim.copy()\n",
    "    qlabels_l1[np.arange(batch_length), labels_beta1] = np.squeeze(qn_l2_targ(np.expand_dims(s_2_batch, axis=0)).numpy())[np.arange(batch_length),opt_a_2_prim]\n",
    "\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_l1_prim.trainable_variables)\n",
    "            pred_q_1s = qn_l1_prim(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0))\n",
    "\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_1s, qlabels_l1)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "            grads = tape.gradient(loss, qn_l1_prim.trainable_variables)\n",
    "\n",
    "            optimizer_ql1.apply_gradients(zip(grads, qn_l1_prim.trainable_variables))\n",
    "            \n",
    "    s_2_batch = np.array([[v[0], v[2]] for v in batch])\n",
    "    s_3_batch = np.array([[v[0], v[1], v[2], v[3]] for v in batch])\n",
    "\n",
    "    #labels_guess = np.array([v[7] for v in batch])\n",
    "    labels_action_2 = np.array([v[5] for v in batch])\n",
    "\n",
    "    q_3_prim = qn_guess_prim(np.expand_dims(s_3_batch, axis=0))\n",
    "    q_3_prim = np.squeeze(q_3_prim.numpy())\n",
    "\n",
    "    opt_a_3_prim = np.argmax(q_3_prim, axis=1)\n",
    "\n",
    "    update_for_q_2_prim = qn_l2_targ(np.expand_dims(s_2_batch, axis=0))\n",
    "    update_for_q_2_prim = np.squeeze(update_for_q_2_prim, axis=0)\n",
    "    qlabels_l2 = update_for_q_2_prim.copy()\n",
    "    qlabels_l2[np.arange(batch_length), labels_action_2] = np.squeeze(qn_guess_targ(np.expand_dims(s_3_batch, axis=0)).numpy())[np.arange(batch_length), opt_a_3_prim]\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_l2_prim.trainable_variables)\n",
    "            pred_q_2s = qn_l2_prim(np.expand_dims(s_2_batch, axis=0))\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_2s, qlabels_l2)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "            grads = tape.gradient(loss, qn_l2_prim.trainable_variables)\n",
    "            optimizer_ql2.apply_gradients(zip(grads, qn_l2_prim.trainable_variables))\n",
    "\n",
    "\n",
    "    s_3_batch = np.array([[v[0], v[1], v[2], v[3]] for v in batch])\n",
    "    rewards = np.array([v[-1] for v in batch])\n",
    "    labels_guess = np.array([v[7] for v in batch])\n",
    "\n",
    "    update_for_q_3_prim = qn_guess_targ(np.expand_dims(s_3_batch, axis=0))\n",
    "    update_for_q_3_prim = np.squeeze(update_for_q_3_prim, axis=0)\n",
    "    qlabels_l3 = update_for_q_3_prim.copy()\n",
    "    qlabels_l3[np.arange(batch_length), labels_guess] = rewards[np.arange(batch_length)]\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_guess_prim.trainable_variables)\n",
    "            pred_q_3s = qn_guess_prim(np.expand_dims(s_3_batch, axis=0))\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_3s, qlabels_l3)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "            grads = tape.gradient(loss, qn_guess_prim.trainable_variables)\n",
    "            optimizer_ql3.apply_gradients(zip(grads, qn_guess_prim.trainable_variables))\n",
    "            \n",
    "    for t, e in zip(qn_l1_targ.trainable_variables, qn_l1_prim.trainable_variables):\n",
    "        t.assign(t*(1-TAU) + e*TAU)\n",
    "\n",
    "    for t, e in zip(qn_l2_targ.trainable_variables, qn_l2_prim.trainable_variables):\n",
    "        t.assign(t*(1-TAU) + e*TAU)\n",
    "\n",
    "    for t, e in zip(qn_guess_targ.trainable_variables, qn_guess_prim.trainable_variables):\n",
    "        t.assign(t*(1-TAU) + e*TAU)\n",
    "    return\n",
    "    \n",
    "\n",
    "\n",
    "buffer = Memory(10**4)\n",
    "optimizer_ql1 = tf.keras.optimizers.Adam(lr=0.001)\n",
    "optimizer_ql2 = tf.keras.optimizers.Adam(lr=0.001)\n",
    "optimizer_ql3 = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "alpha = .56\n",
    "states_wasted = 10**4\n",
    "TAU = 0.08\n",
    "\n",
    "cumulative = []\n",
    "def main():\n",
    "    cum_rews=0\n",
    "    for episode in range(states_wasted):\n",
    "        if episode%100 == 0:\n",
    "            print(episode, \" of \", states_wasted)\n",
    "\n",
    "        epsilon = np.exp(-0.001*episode)\n",
    "        phase = np.random.choice([-1,1],1)[0]\n",
    "        labelbeta1, beta1 = give_first_beta(epsilon)\n",
    "        p0 = np.exp(-(beta1-(phase*np.cos(ats[0])*alpha))**2)\n",
    "        outcome1 = np.random.choice([0,1],1,p=[p0,1-p0])[0]\n",
    "        new_state = [outcome1, beta1]\n",
    "        labelbeta2, beta2 = give_second_beta(new_state,epsilon)\n",
    "        p1 = np.exp(-(beta2-(phase*np.sin(ats[0])*alpha))**2)\n",
    "        outcome2 = np.random.choice([0,1],1,p=[p1,1-p1])[0]\n",
    "        new_state = [outcome1, outcome2, beta1, beta2]\n",
    "        label_guess, guess = give_guess(new_state,epsilon)\n",
    "        if guess == phase:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        buffer.add_sample((outcome1, outcome2, beta1, beta2, labelbeta1, labelbeta2, guess, label_guess, reward))\n",
    "        if episode > 10**2:\n",
    "            learn()\n",
    "        cum_rews += reward\n",
    "        cumulative.append(cum_rews)\n",
    "    return cumulative\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.5       , 0.33333333, ..., 0.81856371, 0.81858186,\n",
       "       0.8186    ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative/np.arange(1,states_wasted+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7295, 7296, 7297, ..., 9997, 9998, 9999]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(cumulative/np.arange(1,states_wasted+1) > 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f435d7e6240>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cumulative/np.arange(1,states_wasted+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*2*100*2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
