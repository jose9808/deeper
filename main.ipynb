{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import basics\n",
    "import misc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import random\n",
    "\n",
    "basic = basics.Basics(resolution=.1)\n",
    "basic.define_actions()\n",
    "actions = basic.actions\n",
    "ats = misc.make_attenuations(layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._samples = []\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self._max_memory:\n",
    "            self._samples.pop(0)\n",
    "    def sample(self, no_samples):\n",
    "        if no_samples > len(self._samples):\n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else:\n",
    "            return random.sample(self._samples, no_samples)\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinality_betas = len(basic.actions[0])\n",
    "\n",
    "class QN_l1(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_l1,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(0,), kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "        self.l2 = Dense(35, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "\n",
    "        self.l3 = Dense(cardinality_betas, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "class QN_l2(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_l2,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(1,2), kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "        self.l2 = Dense(35, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None))\n",
    "\n",
    "\n",
    "        self.l3 = Dense(cardinality_betas, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "class QN_guess(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN_guess,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(1,4), kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l2 = Dense(35, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "        self.l3 = Dense(2, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        # feat = tf.nn.relu(self.l21(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_first_beta(epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        label = np.random.choice(np.arange(len(basic.actions[0])))\n",
    "        return label, basic.actions[0][label]\n",
    "    else:\n",
    "        input = np.expand_dims(np.array([]), axis=0)\n",
    "        q1s = qn_l1_prim(input)\n",
    "        q1s = q1s.numpy()\n",
    "        label = np.argmax(q1s)\n",
    "        beta1 = basic.actions[0][label]\n",
    "        return label, beta1\n",
    "\n",
    "def give_second_beta(new_state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        label = np.random.choice(np.arange(len(basic.actions[1])))\n",
    "        return label, basic.actions[1][label]\n",
    "    else:\n",
    "        input = np.expand_dims(np.array(new_state), axis=0)\n",
    "        q2s = qn_l2_prim(input)\n",
    "        q2s = q2s.numpy()\n",
    "        label = np.argmax(q2s)\n",
    "        beta2 = basic.actions[1][label]\n",
    "        return label, beta2\n",
    "\n",
    "\n",
    "def give_guess(new_state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        guess = np.random.choice(basic.possible_phases,1)[0]\n",
    "        return int((guess+1)/2), guess\n",
    "    else:\n",
    "        input = np.expand_dims(np.array(new_state), axis=0)\n",
    "        qguess = qn_guess_prim(input)\n",
    "        guess = qguess.numpy()\n",
    "        label = np.argmax(guess)\n",
    "        guess = basic.possible_phases[label]\n",
    "        return int((guess+1)/2), guess\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(batch_length):\n",
    "    batch = buffer.sample(batch_length)\n",
    "\n",
    "    s_2_batch = np.array([[ v[0], v[2]] for v in batch ] )\n",
    "    labels_beta1 = np.array([v[4] for v in batch])\n",
    "\n",
    "    q_2_prim = qn_l2_prim(np.expand_dims(s_2_batch, axis=0))\n",
    "    q_2_prim = np.squeeze(q_2_prim.numpy())\n",
    "\n",
    "    opt_a_2_prim = np.argmax(q_2_prim,axis=1)\n",
    "\n",
    "    update_for_q_1_prim = qn_l1_targ(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0)) #targ = target\n",
    "    update_for_q_1_prim = np.squeeze(update_for_q_1_prim, axis=0)\n",
    "    qlabels_l1 = update_for_q_1_prim.copy()\n",
    "    qlabels_l1[np.arange(batch_length), labels_beta1] = np.squeeze(qn_l2_targ(np.expand_dims(s_2_batch, axis=0)).numpy())[np.arange(batch_length),opt_a_2_prim]\n",
    "\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_l1_prim.trainable_variables)\n",
    "            pred_q_1s = qn_l1_prim(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0))\n",
    "\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_1s, qlabels_l1)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "            grads = tape.gradient(loss, qn_l1_prim.trainable_variables)\n",
    "\n",
    "            optimizer_ql1.apply_gradients(zip(grads, qn_l1_prim.trainable_variables))\n",
    "            \n",
    "    s_2_batch = np.array([[v[0], v[2]] for v in batch])\n",
    "    s_3_batch = np.array([[v[0], v[1], v[2], v[3]] for v in batch])\n",
    "\n",
    "    #labels_guess = np.array([v[7] for v in batch])\n",
    "    labels_action_2 = np.array([v[5] for v in batch])\n",
    "\n",
    "    q_3_prim = qn_guess_prim(np.expand_dims(s_3_batch, axis=0))\n",
    "    q_3_prim = np.squeeze(q_3_prim.numpy())\n",
    "\n",
    "    opt_a_3_prim = np.argmax(q_3_prim, axis=1)\n",
    "\n",
    "    update_for_q_2_prim = qn_l2_targ(np.expand_dims(s_2_batch, axis=0))\n",
    "    update_for_q_2_prim = np.squeeze(update_for_q_2_prim, axis=0)\n",
    "    qlabels_l2 = update_for_q_2_prim.copy()\n",
    "    qlabels_l2[np.arange(batch_length), labels_action_2] = np.squeeze(qn_guess_targ(np.expand_dims(s_3_batch, axis=0)).numpy())[np.arange(batch_length), opt_a_3_prim]\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_l2_prim.trainable_variables)\n",
    "            pred_q_2s = qn_l2_prim(np.expand_dims(s_2_batch, axis=0))\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_2s, qlabels_l2)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "            grads = tape.gradient(loss, qn_l2_prim.trainable_variables)\n",
    "            optimizer_ql2.apply_gradients(zip(grads, qn_l2_prim.trainable_variables))\n",
    "\n",
    "\n",
    "    s_3_batch = np.array([[v[0], v[1], v[2], v[3]] for v in batch])\n",
    "    rewards = np.array([v[-1] for v in batch])\n",
    "    labels_guess = np.array([v[7] for v in batch])\n",
    "\n",
    "    update_for_q_3_prim = qn_guess_targ(np.expand_dims(s_3_batch, axis=0))\n",
    "    update_for_q_3_prim = np.squeeze(update_for_q_3_prim, axis=0)\n",
    "    qlabels_l3 = update_for_q_3_prim.copy()\n",
    "    qlabels_l3[np.arange(batch_length), labels_guess] = rewards[np.arange(batch_length)]\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_guess_prim.trainable_variables)\n",
    "            pred_q_3s = qn_guess_prim(np.expand_dims(s_3_batch, axis=0))\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_3s, qlabels_l3)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "\n",
    "            grads = tape.gradient(loss, qn_guess_prim.trainable_variables)\n",
    "            optimizer_ql3.apply_gradients(zip(grads, qn_guess_prim.trainable_variables))\n",
    "            \n",
    "    for t, e in zip(qn_l1_targ.trainable_variables, qn_l1_prim.trainable_variables):\n",
    "        t.assign(t*(1-TAU) + e*TAU)\n",
    "\n",
    "    for t, e in zip(qn_l2_targ.trainable_variables, qn_l2_prim.trainable_variables):\n",
    "        t.assign(t*(1-TAU) + e*TAU)\n",
    "\n",
    "    for t, e in zip(qn_guess_targ.trainable_variables, qn_guess_prim.trainable_variables):\n",
    "        t.assign(t*(1-TAU) + e*TAU)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(beta, alpha, n):\n",
    "    p0 = np.exp(-(beta-alpha)**2)\n",
    "    if n == 0:\n",
    "        return p0\n",
    "    else:\n",
    "        return 1-p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_greedy():\n",
    "    p=0\n",
    "    label1, beta1 = give_first_beta(epsilon=0)\n",
    "    for n1 in [0,1]:\n",
    "        for n2 in [0,1]:\n",
    "            l2, beta2 = give_second_beta([n1, beta1], epsilon=0)\n",
    "            gueslabel, guess_phase = give_guess([n1,n2,beta1,beta2], epsilon=0)\n",
    "            p+= prob(beta1, guess_phase*np.cos(ats[0])*alpha,n1)*prob(beta2, guess_phase*np.sin(ats[0])*alpha, n2)\n",
    "    return p/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(states_wasted=10**4):\n",
    "    buffer = Memory(states_wasted)\n",
    "    optimizer_ql1 = tf.keras.optimizers.SGD(lr=0.00001)\n",
    "    optimizer_ql2 = tf.keras.optimizers.SGD(lr=0.00001)\n",
    "    optimizer_ql3 = tf.keras.optimizers.SGD(lr=0.00001)\n",
    "\n",
    "    alpha = .56\n",
    "    TAU = 0.001\n",
    "    \n",
    "    #### define the networks #####\n",
    "\n",
    "    qn_l1_prim = QN_l1()\n",
    "    qn_l1_targ = QN_l1()\n",
    "\n",
    "    qn_l2_prim = QN_l2()\n",
    "    qn_l2_targ = QN_l2()\n",
    "\n",
    "    qn_guess_prim = QN_guess()\n",
    "    qn_guess_targ = QN_guess()\n",
    "\n",
    "\n",
    "\n",
    "    cumulative = []\n",
    "    success_prob_evolution = []\n",
    "    cum_rews=0\n",
    "    for episode in range(states_wasted):\n",
    "        if episode%100 == 0:\n",
    "            print(episode, \" of \", states_wasted)\n",
    "            if episode>1:\n",
    "                print(\"cumulative: \", str(cumulative[-1]/episode))\n",
    "                print(\"p_s_greedy: \", str(probability_greedy()))\n",
    "        epsilon = max(0.01, np.exp(-episode/500))\n",
    "        phase = np.random.choice([-1,1],1)[0]\n",
    "        labelbeta1, beta1 = give_first_beta(epsilon)\n",
    "        p0 = np.exp(-(beta1-(phase*np.cos(ats[0])*alpha))**2)\n",
    "        outcome1 = np.random.choice([0,1],1,p=[p0,1-p0])[0]\n",
    "        new_state = [outcome1, beta1]\n",
    "        labelbeta2, beta2 = give_second_beta(new_state,epsilon)\n",
    "        p1 = np.exp(-(beta2-(phase*np.sin(ats[0])*alpha))**2)\n",
    "        outcome2 = np.random.choice([0,1],1,p=[p1,1-p1])[0]\n",
    "        new_state = [outcome1, outcome2, beta1, beta2]\n",
    "        label_guess, guess = give_guess(new_state,epsilon)\n",
    "        if guess == phase:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        buffer.add_sample((outcome1, outcome2, beta1, beta2, labelbeta1, labelbeta2, guess, label_guess, reward))\n",
    "        if episode > 1:\n",
    "            learn(batch_length=episode)\n",
    "        cum_rews += reward\n",
    "        success_prob_evolution.append(probability_greedy())\n",
    "        cumulative.append(cum_rews)\n",
    "        #if episode%10**3 == 0:\n",
    "         #   save_parameters()\n",
    "    return np.arange(1, states_wasted+1), cumulative, success_prob_evolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  of  10000000\n",
      "100  of  10000000\n",
      "cumulative:  0.42\n",
      "p_s_greedy:  0.8093497940143703\n",
      "200  of  10000000\n",
      "cumulative:  0.48\n",
      "p_s_greedy:  0.8093497940143703\n",
      "300  of  10000000\n",
      "cumulative:  0.52\n",
      "p_s_greedy:  0.8093497940143703\n",
      "400  of  10000000\n",
      "cumulative:  0.545\n",
      "p_s_greedy:  0.8093497940143703\n",
      "500  of  10000000\n",
      "cumulative:  0.542\n",
      "p_s_greedy:  0.8093497940143703\n",
      "600  of  10000000\n",
      "cumulative:  0.55\n",
      "p_s_greedy:  0.8093497940143703\n",
      "700  of  10000000\n",
      "cumulative:  0.5757142857142857\n",
      "p_s_greedy:  0.8093497940143703\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (781,) (780,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-b6b8eac6cee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-127-1556041fdb46>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(states_wasted)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutcome1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcome2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_guess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mcum_rews\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0msuccess_prob_evolution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-90e6c5ec0ddf>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(batch_length)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mupdate_for_q_1_prim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_for_q_1_prim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mqlabels_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_for_q_1_prim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mqlabels_l1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_beta1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqn_l2_targ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_a_2_prim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (781,) (780,) "
     ]
    }
   ],
   "source": [
    "times, cums, probs = main(10**7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=[]\n",
    "for i in basic.actions[0]:\n",
    "    for j1 in basic.actions[1]:\n",
    "        for j2 in basic.actions[1]:\n",
    "            f.append(basic.probability_error([i, j1, j2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9022006565141967"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.min(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9\n",
      "-2.220446049250313e-16\n",
      "-0.30000000000000016\n",
      "-0.1000000000000002\n",
      "-1.0\n",
      "-2.220446049250313e-16\n",
      "-0.9\n",
      "-0.1000000000000002\n",
      "-0.9\n",
      "-0.8\n",
      "-0.20000000000000018\n",
      "-0.5000000000000001\n",
      "-0.6000000000000001\n",
      "-2.220446049250313e-16\n",
      "-0.1000000000000002\n",
      "-0.30000000000000016\n",
      "-0.40000000000000013\n",
      "-0.30000000000000016\n",
      "-0.20000000000000018\n",
      "-0.8\n",
      "-0.40000000000000013\n",
      "-0.7000000000000001\n",
      "-0.5000000000000001\n",
      "-0.30000000000000016\n",
      "-0.40000000000000013\n",
      "-1.0\n",
      "-0.9\n",
      "-0.1000000000000002\n",
      "-0.30000000000000016\n",
      "-0.40000000000000013\n",
      "-2.220446049250313e-16\n",
      "-0.9\n",
      "-2.220446049250313e-16\n",
      "-1.0\n",
      "-0.40000000000000013\n",
      "-0.20000000000000018\n",
      "-0.1000000000000002\n",
      "-2.220446049250313e-16\n",
      "-0.40000000000000013\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-0.20000000000000018\n",
      "-0.20000000000000018\n",
      "-1.0\n",
      "-2.220446049250313e-16\n",
      "-0.5000000000000001\n",
      "-0.5000000000000001\n",
      "-0.6000000000000001\n",
      "-0.40000000000000013\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(give_first_beta(epsilon=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.20000000000000018\n",
      "-0.6000000000000001\n",
      "-1.0\n",
      "0.7999999999999996\n",
      "0.9999999999999996\n",
      "0.5999999999999996\n",
      "0.19999999999999973\n",
      "0.5999999999999996\n",
      "-0.20000000000000018\n",
      "-1.0\n",
      "0.5999999999999996\n",
      "-0.8\n",
      "-1.0\n",
      "0.19999999999999973\n",
      "-0.20000000000000018\n",
      "0.9999999999999996\n",
      "0.9999999999999996\n",
      "-0.6000000000000001\n",
      "0.19999999999999973\n",
      "0.7999999999999996\n",
      "0.9999999999999996\n",
      "0.5999999999999996\n",
      "-0.6000000000000001\n",
      "0.9999999999999996\n",
      "0.5999999999999996\n",
      "-0.8\n",
      "-0.40000000000000013\n",
      "-1.0\n",
      "-0.6000000000000001\n",
      "-0.8\n",
      "0.9999999999999996\n",
      "0.7999999999999996\n",
      "-0.6000000000000001\n",
      "0.3999999999999997\n",
      "-0.40000000000000013\n",
      "-0.40000000000000013\n",
      "-0.6000000000000001\n",
      "0.5999999999999996\n",
      "-0.8\n",
      "0.9999999999999996\n",
      "-0.20000000000000018\n",
      "-0.8\n",
      "0.3999999999999997\n",
      "0.3999999999999997\n",
      "-0.20000000000000018\n",
      "-1.0\n",
      "-2.220446049250313e-16\n",
      "-0.20000000000000018\n",
      "-0.6000000000000001\n",
      "0.19999999999999973\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(give_second_beta([0,-.77],epsilon=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "0.5999999999999996\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-0.8\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(give_second_beta([1,-.77],epsilon=0.01)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(give_second_beta([43,-.77],epsilon=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
