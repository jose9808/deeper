{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from memory import Memory\n",
    "from basics import Basics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from networks_kennedy import QN_l1, QN_guess_kennedy\n",
    "import give_probability_kennedy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "basics = Basics(layers=1)\n",
    "basics.define_actions()\n",
    "b =np.load(\"buffer.npy\", allow_pickle=True)\n",
    "buffer = Memory(len(b), load_path=None)\n",
    "for i in range(len(b)):\n",
    "    buffer.add_sample(tuple(b[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1108 21:39:12.026978 139891461576512 base_layer.py:1865] Layer qn_l1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1108 21:39:12.079481 139891461576512 base_layer.py:1865] Layer qn_l1_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1108 21:39:12.093730 139891461576512 base_layer.py:1865] Layer qn_guess_kennedy is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1108 21:39:12.107048 139891461576512 base_layer.py:1865] Layer qn_guess_kennedy_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_batch(TAU, networks , batch_length, optimizers):\n",
    "    batch = buffer.sample(batch_length)\n",
    "    qn_l1_prim, qn_l1_targ, qn_guess_prim, qn_guess_targ = networks\n",
    "\n",
    "    outcome_1_beta1_batch = np.array([[ int(v[0]), v[1]] for v in batch ] )\n",
    "    labels_beta1 = np.array([int(v[2]) for v in batch])\n",
    "\n",
    "    q_guess_prim = qn_guess_prim(np.expand_dims(outcome_1_beta1_batch, axis=0))\n",
    "    q_guess_prim = np.squeeze(q_guess_prim.numpy())\n",
    "\n",
    "    opt_a_2_prim = np.argmax(q_guess_prim,axis=1)\n",
    "\n",
    "    update_for_q_1_prim = qn_l1_targ(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0)) #targ = target\n",
    "    update_for_q_1_prim = np.squeeze(update_for_q_1_prim, axis=0)\n",
    "    qlabels_l1 = update_for_q_1_prim.copy()\n",
    "\n",
    "    # print(qlabels_l1.shape, type(qlabels_l1), batch_length, np.arange(batch_length))\n",
    "    np.save(\"labelsbeta1\", labels_beta1)\n",
    "    qlabels_l1[np.arange(batch_length), labels_beta1] = np.squeeze(qn_guess_targ(np.expand_dims(outcome_1_beta1_batch, axis=0)).numpy())[np.arange(batch_length),opt_a_2_prim]\n",
    "\n",
    "    optimizer_ql1, optimizer_guess = optimizers\n",
    "\n",
    "    train_vars_in = [ch.numpy() for ch in qn_l1_prim.trainable_variables]\n",
    "\n",
    "    # print(qlabels_l1)\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_l1_prim.trainable_variables)\n",
    "            pred_q_1s = qn_l1_prim(np.expand_dims(np.array([[] for i in range(len(batch))]), axis=0))\n",
    "\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_1s, qlabels_l1)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "            grads = tape.gradient(loss, qn_l1_prim.trainable_variables)\n",
    "            optimizer_ql1.apply_gradients(zip(grads, qn_l1_prim.trainable_variables))\n",
    "\n",
    "        s_3_batch = np.array([[int(v[0]), v[1] ] for v in batch])\n",
    "        rewards = np.array([v[-1] for v in batch])\n",
    "        labels_guess = np.array([int(v[-2]) for v in batch])\n",
    "\n",
    "        update_for_q_3_prim = qn_guess_targ(np.expand_dims(s_3_batch, axis=0))\n",
    "        update_for_q_3_prim = np.squeeze(update_for_q_3_prim, axis=0)\n",
    "        qlabels_l3 = update_for_q_3_prim.copy()\n",
    "        qlabels_l3[np.arange(batch_length), labels_guess] = rewards[np.arange(batch_length)]\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(qn_guess_prim.trainable_variables)\n",
    "            pred_q_3s = qn_guess_prim(np.expand_dims(s_3_batch, axis=0))\n",
    "            loss_sum =tf.keras.losses.MSE(pred_q_3s, qlabels_l3)\n",
    "            loss = tf.reduce_mean(loss_sum)\n",
    "            loss_guess = loss\n",
    "            grads = tape.gradient(loss, qn_guess_prim.trainable_variables)\n",
    "            optimizer_guess.apply_gradients(zip(grads, qn_guess_prim.trainable_variables))\n",
    "\n",
    "        for t, e in zip(qn_l1_targ.trainable_variables, qn_l1_prim.trainable_variables):\n",
    "            t.assign(t*(1-TAU) + e*TAU)\n",
    "\n",
    "        for t, e in zip(qn_guess_targ.trainable_variables, qn_guess_prim.trainable_variables):\n",
    "            t.assign(t*(1-TAU) + e*TAU)\n",
    "    return loss, loss_guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 22:09:16.989151 139891461576512 base_layer.py:1865] Layer qn_l1_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1108 22:09:17.003111 139891461576512 base_layer.py:1865] Layer qn_l1_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1108 22:09:17.016964 139891461576512 base_layer.py:1865] Layer qn_guess_kennedy_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1108 22:09:17.028024 139891461576512 base_layer.py:1865] Layer qn_guess_kennedy_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1\n",
      "loss_l1:  tf.Tensor(0.35994396, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35994396, shape=(), dtype=float32)\n",
      "episode:  101\n",
      "loss_l1:  tf.Tensor(0.36067376, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.36067376, shape=(), dtype=float32)\n",
      "episode:  201\n",
      "loss_l1:  tf.Tensor(0.36235538, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.36235538, shape=(), dtype=float32)\n",
      "episode:  301\n",
      "loss_l1:  tf.Tensor(0.35314012, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35314012, shape=(), dtype=float32)\n",
      "episode:  401\n",
      "loss_l1:  tf.Tensor(0.35525572, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35525572, shape=(), dtype=float32)\n",
      "episode:  501\n",
      "loss_l1:  tf.Tensor(0.35585225, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35585225, shape=(), dtype=float32)\n",
      "episode:  601\n",
      "loss_l1:  tf.Tensor(0.36008358, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.36008358, shape=(), dtype=float32)\n",
      "episode:  701\n",
      "loss_l1:  tf.Tensor(0.36376137, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.36376137, shape=(), dtype=float32)\n",
      "episode:  801\n",
      "loss_l1:  tf.Tensor(0.35395834, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35395834, shape=(), dtype=float32)\n",
      "episode:  901\n",
      "loss_l1:  tf.Tensor(0.36013523, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.36013523, shape=(), dtype=float32)\n",
      "episode:  1001\n",
      "loss_l1:  tf.Tensor(0.35718706, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35718706, shape=(), dtype=float32)\n",
      "episode:  1101\n",
      "loss_l1:  tf.Tensor(0.35294992, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35294992, shape=(), dtype=float32)\n",
      "episode:  1201\n",
      "loss_l1:  tf.Tensor(0.35633358, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35633358, shape=(), dtype=float32)\n",
      "episode:  1301\n",
      "loss_l1:  tf.Tensor(0.35677963, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35677963, shape=(), dtype=float32)\n",
      "episode:  1401\n",
      "loss_l1:  tf.Tensor(0.35566008, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35566008, shape=(), dtype=float32)\n",
      "episode:  1501\n",
      "loss_l1:  tf.Tensor(0.35532427, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35532427, shape=(), dtype=float32)\n",
      "episode:  1601\n",
      "loss_l1:  tf.Tensor(0.35913143, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35913143, shape=(), dtype=float32)\n",
      "episode:  1701\n",
      "loss_l1:  tf.Tensor(0.3627188, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.3627188, shape=(), dtype=float32)\n",
      "episode:  1801\n",
      "loss_l1:  tf.Tensor(0.35988387, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35988387, shape=(), dtype=float32)\n",
      "episode:  1901\n",
      "loss_l1:  tf.Tensor(0.35920113, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35920113, shape=(), dtype=float32)\n",
      "episode:  2001\n",
      "loss_l1:  tf.Tensor(0.35387027, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35387027, shape=(), dtype=float32)\n",
      "episode:  2101\n",
      "loss_l1:  tf.Tensor(0.35709646, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35709646, shape=(), dtype=float32)\n",
      "episode:  2201\n",
      "loss_l1:  tf.Tensor(0.35573187, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35573187, shape=(), dtype=float32)\n",
      "episode:  2301\n",
      "loss_l1:  tf.Tensor(0.36017752, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.36017752, shape=(), dtype=float32)\n",
      "episode:  2401\n",
      "loss_l1:  tf.Tensor(0.36189654, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.36189654, shape=(), dtype=float32)\n",
      "episode:  2501\n",
      "loss_l1:  tf.Tensor(0.35020202, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35020202, shape=(), dtype=float32)\n",
      "episode:  2601\n",
      "loss_l1:  tf.Tensor(0.35559306, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35559306, shape=(), dtype=float32)\n",
      "episode:  2701\n",
      "loss_l1:  tf.Tensor(0.35510722, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35510722, shape=(), dtype=float32)\n",
      "episode:  2801\n",
      "loss_l1:  tf.Tensor(0.3644501, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.3644501, shape=(), dtype=float32)\n",
      "episode:  2901\n",
      "loss_l1:  tf.Tensor(0.35954076, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.35954076, shape=(), dtype=float32)\n",
      "episode:  3001\n",
      "loss_l1:  tf.Tensor(0.34814933, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.34814933, shape=(), dtype=float32)\n",
      "episode:  3101\n",
      "loss_l1:  tf.Tensor(0.3497018, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.3497018, shape=(), dtype=float32)\n",
      "episode:  3201\n",
      "loss_l1:  tf.Tensor(0.34527856, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.34527856, shape=(), dtype=float32)\n",
      "episode:  3301\n",
      "loss_l1:  tf.Tensor(0.3407904, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.3407904, shape=(), dtype=float32)\n",
      "episode:  3401\n",
      "loss_l1:  tf.Tensor(0.34302744, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.34302744, shape=(), dtype=float32)\n",
      "episode:  3501\n",
      "loss_l1:  tf.Tensor(0.33758178, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.33758178, shape=(), dtype=float32)\n",
      "episode:  3601\n",
      "loss_l1:  tf.Tensor(0.33560634, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.33560634, shape=(), dtype=float32)\n",
      "episode:  3701\n",
      "loss_l1:  tf.Tensor(0.33972663, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.33972663, shape=(), dtype=float32)\n",
      "episode:  3801\n",
      "loss_l1:  tf.Tensor(0.32991982, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.32991982, shape=(), dtype=float32)\n",
      "episode:  3901\n",
      "loss_l1:  tf.Tensor(0.32002804, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.32002804, shape=(), dtype=float32)\n",
      "episode:  4001\n",
      "loss_l1:  tf.Tensor(0.32968292, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.32968292, shape=(), dtype=float32)\n",
      "episode:  4101\n",
      "loss_l1:  tf.Tensor(0.3125227, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.3125227, shape=(), dtype=float32)\n",
      "episode:  4201\n",
      "loss_l1:  tf.Tensor(0.30401334, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.30401334, shape=(), dtype=float32)\n",
      "episode:  4301\n",
      "loss_l1:  tf.Tensor(0.30428487, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.30428487, shape=(), dtype=float32)\n",
      "episode:  4401\n",
      "loss_l1:  tf.Tensor(0.29610035, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.29610035, shape=(), dtype=float32)\n",
      "episode:  4501\n",
      "loss_l1:  tf.Tensor(0.29118448, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.29118448, shape=(), dtype=float32)\n",
      "episode:  4601\n",
      "loss_l1:  tf.Tensor(0.2782686, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.2782686, shape=(), dtype=float32)\n",
      "episode:  4701\n",
      "loss_l1:  tf.Tensor(0.2815876, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.2815876, shape=(), dtype=float32)\n",
      "episode:  4801\n",
      "loss_l1:  tf.Tensor(0.27867132, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.27867132, shape=(), dtype=float32)\n",
      "episode:  4901\n",
      "loss_l1:  tf.Tensor(0.27510604, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.27510604, shape=(), dtype=float32)\n",
      "episode:  5001\n",
      "loss_l1:  tf.Tensor(0.26898676, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.26898676, shape=(), dtype=float32)\n",
      "episode:  5101\n",
      "loss_l1:  tf.Tensor(0.26676792, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.26676792, shape=(), dtype=float32)\n",
      "episode:  5201\n",
      "loss_l1:  tf.Tensor(0.26974723, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.26974723, shape=(), dtype=float32)\n",
      "episode:  5301\n",
      "loss_l1:  tf.Tensor(0.2584932, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.2584932, shape=(), dtype=float32)\n",
      "episode:  5401\n",
      "loss_l1:  tf.Tensor(0.2545303, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.2545303, shape=(), dtype=float32)\n",
      "episode:  5501\n",
      "loss_l1:  tf.Tensor(0.2538609, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.2538609, shape=(), dtype=float32)\n",
      "episode:  5601\n",
      "loss_l1:  tf.Tensor(0.25403258, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.25403258, shape=(), dtype=float32)\n",
      "episode:  5701\n",
      "loss_l1:  tf.Tensor(0.25310367, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.25310367, shape=(), dtype=float32)\n",
      "episode:  5801\n",
      "loss_l1:  tf.Tensor(0.25858593, shape=(), dtype=float32)\n",
      "loss_guess:  tf.Tensor(0.25858593, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-c4f12599f7a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss_l1: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_l1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss_guess: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_guess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTAU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mloss_l1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mloss_guess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-54e8801a01bf>\u001b[0m in \u001b[0;36mlearn_batch\u001b[0;34m(TAU, networks, batch_length, optimizers)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlearn_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTAU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetworks\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mqn_l1_prim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqn_l1_targ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqn_guess_prim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqn_guess_targ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutcome_1_beta1_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cooper-cooper/Desktop/deeper/memory.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, no_samples)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"buffer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mselected_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "LR=0.0005\n",
    "TAU=0.0005\n",
    "\n",
    "\n",
    "qn_l1_prim = QN_l1(basics.actions[0])\n",
    "qn_l1_targ = QN_l1(basics.actions[0])\n",
    "qn_guess_prim = QN_guess_kennedy(basics.possible_phases)\n",
    "qn_guess_targ = QN_guess_kennedy(basics.possible_phases)\n",
    "\n",
    "qvalues1 = qn_l1_prim(np.expand_dims(np.array([]), axis=0))\n",
    "qvalues1targ = qn_l1_targ(np.expand_dims(np.array([]), axis=0))\n",
    "qvalueslayerguess_prim = qn_guess_prim(np.expand_dims(np.array([1,-.7]), axis=0))\n",
    "qvalueslayerguess_targ = qn_guess_targ(np.expand_dims(np.array([1,-.7]), axis=0))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=LR)\n",
    "optimizer_guess = tf.keras.optimizers.SGD(lr=LR)\n",
    "\n",
    "networks = [qn_l1_prim, qn_l1_targ, qn_guess_prim, qn_guess_targ]\n",
    "optimizers = [optimizer, optimizer_guess]\n",
    "\n",
    "qn_l1_prim.give_first_beta(epsilon=1)\n",
    "qn_l1_targ.give_first_beta(epsilon=1)\n",
    "qn_guess_prim.give_guess([0, -.7], epsilon=1)\n",
    "qn_guess_targ.give_guess([0, -.7], epsilon=1)\n",
    "\n",
    "loss_l1 = []\n",
    "loss_guess = []\n",
    "for epoch in range(10**6):\n",
    "    if epoch%10**2 == 1:\n",
    "        print(\"episode: \",epoch )\n",
    "        print(\"loss_l1: \",loss_l1[-1])\n",
    "        print(\"loss_guess: \",loss_guess[-1])\n",
    "    losses = learn_batch(TAU, networks, 10000, optimizers)\n",
    "    loss_l1.append(losses[0])\n",
    "    loss_guess.append(losses[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvalues1 = qn_l1_prim(np.expand_dims(np.array([]), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3ab008cc50>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(qvalues1.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (5812,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f2b472504717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_guess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cooper-cooper/.local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     return gca().plot(\n\u001b[1;32m   2794\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2795\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cooper-cooper/.local/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cooper-cooper/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cooper-cooper/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cooper-cooper/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (5812,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "times = np.arange(10**3)\n",
    "plt.plot(times, loss_guess)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
