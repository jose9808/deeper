{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\n",
      "8192/2194 [================================================================================================================] - 0s 3us/step\n",
      "Local copy of the dataset file: /home/cooper-cooper/.keras/datasets/iris_training.csv\n"
     ]
    }
   ],
   "source": [
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                           origin=train_dataset_url)\n",
    "\n",
    "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120,4,setosa,versicolor,virginica\r\n",
      "6.4,2.8,5.6,2.2,2\r\n",
      "5.0,2.3,3.3,1.0,1\r\n",
      "4.9,2.5,4.5,1.7,2\r\n",
      "4.9,3.1,1.5,0.1,0\r\n"
     ]
    }
   ],
   "source": [
    "! head -n5 {train_dataset_fp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(train_dataset_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['120', '4', 'setosa', 'versicolor', 'virginica'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column:  setosa\n",
      "mean:  3.739166666666667\n",
      "min-max:  1.0 - 6.9\n",
      "\n",
      "column:  versicolor\n",
      "mean:  1.1966666666666665\n",
      "min-max:  0.1 - 2.5\n",
      "\n",
      "column:  virginica\n",
      "mean:  1.0\n",
      "min-max:  0 - 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in data.columns[2:5]:\n",
    "    print(\"column: \",k)\n",
    "    print(\"mean: \", data[k].mean())\n",
    "    print(\"min-max: \", data[k].min(), \"-\", data[k].max())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.9"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"setosa\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above is wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Label: species\n"
     ]
    }
   ],
   "source": [
    "# column order in CSV file\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "print(\"Label: {}\".format(label_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cooper-cooper/.keras/datasets/iris_training.csv'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    train_dataset_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    train_dataset_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name=label_name,\n",
    "    num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('sepal_length', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([6.9, 5.7, 5.5, 5. , 5.9, 6.9, 5.5, 5.1, 5.2, 7.2, 4.7, 5. , 6.2,\n",
      "       7.7, 5. , 4.9, 7.4, 5.4, 4.9, 4.8, 5.8, 5.8, 6.5, 6.3, 6.1, 5.4,\n",
      "       5. , 5. , 5.7, 6. , 7.3, 4.4], dtype=float32)>), ('sepal_width', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([3.2, 2.8, 3.5, 3.5, 3.2, 3.1, 2.4, 3.8, 2.7, 3.6, 3.2, 3.4, 2.8,\n",
      "       2.6, 3. , 2.5, 2.8, 3.9, 3.1, 3. , 2.8, 4. , 3.2, 2.3, 3. , 3.7,\n",
      "       3.6, 2. , 4.4, 2.7, 2.9, 3. ], dtype=float32)>), ('petal_length', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([5.7, 4.1, 1.3, 1.3, 4.8, 4.9, 3.7, 1.5, 3.9, 6.1, 1.3, 1.6, 4.8,\n",
      "       6.9, 1.6, 4.5, 6.1, 1.7, 1.5, 1.4, 5.1, 1.2, 5.1, 4.4, 4.9, 1.5,\n",
      "       1.4, 3.5, 1.5, 5.1, 6.3, 1.3], dtype=float32)>), ('petal_width', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([2.3, 1.3, 0.2, 0.3, 1.8, 1.5, 1. , 0.3, 1.4, 2.5, 0.2, 0.4, 1.8,\n",
      "       2.3, 0.2, 1.7, 1.9, 0.4, 0.1, 0.1, 2.4, 0.2, 2. , 1.3, 1.8, 0.2,\n",
      "       0.2, 1. , 0.4, 1.6, 1.8, 0.2], dtype=float32)>)])\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataset))\n",
    "\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i,k in enumerate(train_dataset):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    train_dataset_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name=label_name,\n",
    "    num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat, lab = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sepal_length', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([5. , 4.9, 5.2, 5.5, 7.4, 4.8, 7. , 6.7, 6.9, 6. , 4.6, 6.3, 5. ,\n",
      "       5.7, 4.8, 7.2, 6.1, 4.8, 5.1, 5.8, 5.1, 5.6, 6.5, 6.8, 5.4, 7.7,\n",
      "       7.7, 5.1, 5.6, 5. , 7.7, 6.4], dtype=float32)>)\n",
      "('sepal_width', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([3.4, 3.1, 3.4, 2.4, 2.8, 3.4, 3.2, 3.3, 3.2, 2.7, 3.6, 2.3, 2. ,\n",
      "       2.8, 3. , 3. , 2.6, 3.1, 3.8, 2.7, 3.7, 2.9, 3. , 3.2, 3.7, 3.8,\n",
      "       2.8, 3.5, 2.7, 3.3, 2.6, 3.2], dtype=float32)>)\n",
      "('petal_length', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([1.5, 1.5, 1.4, 3.7, 6.1, 1.6, 4.7, 5.7, 5.7, 5.1, 1. , 4.4, 3.5,\n",
      "       4.5, 1.4, 5.8, 5.6, 1.6, 1.9, 4.1, 1.5, 3.6, 5.8, 5.9, 1.5, 6.7,\n",
      "       6.7, 1.4, 4.2, 1.4, 6.9, 5.3], dtype=float32)>)\n",
      "('petal_width', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([0.2, 0.1, 0.2, 1. , 1.9, 0.2, 1.4, 2.1, 2.3, 1.6, 0.2, 1.3, 1. ,\n",
      "       1.3, 0.3, 1.6, 1.4, 0.2, 0.4, 1. , 0.4, 1.3, 2.2, 2.3, 0.2, 2.2,\n",
      "       2. , 0.3, 1.3, 0.2, 2.3, 2.3], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for k in feat.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal_length', 'sepal_width', 'petal_length', 'petal_width']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0322 18:11:38.569694 140131307001664 legend.py:1282] No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features['petal_length'],\n",
    "            features['sepal_length'],\n",
    "            c=labels,\n",
    "            cmap='viridis')\n",
    "\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Sepal length\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features['petal_length'],\n",
    "            features['sepal_length'],\n",
    "            c=labels,\n",
    "            cmap='viridis')\n",
    "\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Sepal length\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       "array([2, 1, 0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0,\n",
       "       2, 1, 2, 0, 0, 1, 0, 1, 2, 0], dtype=int32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 4), dtype=float32, numpy=\n",
       "array([[6.9, 3.2, 5.7, 2.3],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [4.4, 3. , 1.3, 0.2]], dtype=float32)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack(list(features.values()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "  \"\"\"Pack the features into a single array.\"\"\"\n",
    "  features = tf.stack(list(features.values()), axis=1)\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 4), dtype=float32, numpy=\n",
       "array([[6.6, 3. , 4.4, 1.4],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(ff)\n",
    "tf.nn.softmax(preds)[0].numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    Labels: [1 1 2 1 1 2 0 1 0 2 1 2 0 2 0 2 0 0 0 1 2 0 2 1 0 1 2 2 2 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction: {}\".format(tf.argmax(preds, axis=1)))\n",
    "print(\"    Labels: {}\".format(ii))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss test: 2.3887667655944824\n"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y, training):\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  y_ = model(x, training=training)\n",
    "\n",
    "  return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "\n",
    "l = loss(model, features, labels, training=False)\n",
    "print(\"Loss test: {}\".format(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Sequential in module tensorflow.python.keras.engine.sequential object:\n",
      "\n",
      "class Sequential(tensorflow.python.keras.engine.training.Model)\n",
      " |  Linear stack of layers.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      layers: list of layers to add to the model.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Optionally, the first layer can receive an `input_shape` argument:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(500,)))\n",
      " |  # Afterwards, we do automatic shape inference:\n",
      " |  model.add(Dense(32))\n",
      " |  \n",
      " |  # This is identical to the following:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_dim=500))\n",
      " |  \n",
      " |  # And to the following:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, batch_input_shape=(None, 500)))\n",
      " |  \n",
      " |  # Note that you can also omit the `input_shape` argument:\n",
      " |  # In that case the model gets built the first time you call `fit` (or other\n",
      " |  # training and evaluation methods).\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32))\n",
      " |  model.add(Dense(32))\n",
      " |  model.compile(optimizer=optimizer, loss=loss)\n",
      " |  # This builds the model for the first time:\n",
      " |  model.fit(x, y, batch_size=32, epochs=10)\n",
      " |  \n",
      " |  # Note that when using this delayed-build pattern (no input shape specified),\n",
      " |  # the model doesn't have any weights until the first call\n",
      " |  # to a training/evaluation method (since it isn't yet built):\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32))\n",
      " |  model.add(Dense(32))\n",
      " |  model.weights  # returns []\n",
      " |  \n",
      " |  # Whereas if you specify the input shape, the model gets built continuously\n",
      " |  # as you are adding layers:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(500,)))\n",
      " |  model.add(Dense(32))\n",
      " |  model.weights  # returns list of length 4\n",
      " |  \n",
      " |  # When using the delayed-build pattern (no input shape specified), you can\n",
      " |  # choose to manually build your model by calling `build(batch_input_shape)`:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32))\n",
      " |  model.add(Dense(32))\n",
      " |  model.build((None, 500))\n",
      " |  model.weights  # returns list of length 4\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      tensorflow.python.keras.engine.training.Model\n",
      " |      tensorflow.python.keras.engine.network.Network\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, layers=None, name=None)\n",
      " |  \n",
      " |  add(self, layer)\n",
      " |      Adds a layer instance on top of the layer stack.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          layer: layer instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: If `layer` is not a layer instance.\n",
      " |          ValueError: In case the `layer` argument does not\n",
      " |              know its input shape.\n",
      " |          ValueError: In case the `layer` argument has\n",
      " |              multiple output tensors, or is already connected\n",
      " |              somewhere else (forbidden in `Sequential` models).\n",
      " |  \n",
      " |  build(self, input_shape=None)\n",
      " |      Builds the model based on input shapes received.\n",
      " |      \n",
      " |      This is to be used for subclassed models, which do not know at instantiation\n",
      " |      time what their inputs look like.\n",
      " |      \n",
      " |      This method only exists for users who want to call `model.build()` in a\n",
      " |      standalone way (as a substitute for calling the model on real data to\n",
      " |      build it). It will never be called by the framework (and thus it will\n",
      " |      never throw unexpected errors in an unrelated workflow).\n",
      " |      \n",
      " |      Args:\n",
      " |       input_shape: Single tuple, TensorShape, or list of shapes, where shapes\n",
      " |           are tuples, integers, or TensorShapes.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError:\n",
      " |          1. In case of invalid user-provided data (not of type tuple,\n",
      " |             list, or TensorShape).\n",
      " |          2. If the model requires call arguments that are agnostic\n",
      " |             to the input shapes (positional or kwarg in call signature).\n",
      " |          3. If not all layers were properly built.\n",
      " |          4. If float type inputs are not supported within the layers.\n",
      " |      \n",
      " |        In each of these cases, the user should build their model by calling it\n",
      " |        on real tensor data.\n",
      " |  \n",
      " |  call(self, inputs, training=None, mask=None)\n",
      " |      Calls the model on new inputs.\n",
      " |      \n",
      " |      In this case `call` just reapplies\n",
      " |      all ops in the graph to the new inputs\n",
      " |      (e.g. build a new computational graph from the provided inputs).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: A tensor or list of tensors.\n",
      " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
      " |            the `Network` in training mode or inference mode.\n",
      " |          mask: A mask or list of masks. A mask can be\n",
      " |              either a tensor or None (no mask).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor if there is a single output, or\n",
      " |          a list of tensors if there are more than one outputs.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  pop(self)\n",
      " |      Removes the last layer in the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: if there are no layers in the model.\n",
      " |  \n",
      " |  predict_classes(self, x, batch_size=32, verbose=0)\n",
      " |      Generate class predictions for the input samples.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: integer.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A numpy array of class predictions.\n",
      " |  \n",
      " |  predict_proba(self, x, batch_size=32, verbose=0)\n",
      " |      Generates class probability predictions for the input samples.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: integer.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Numpy array of probability predictions.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Instantiates a Model from its config (output of `get_config()`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: Model config dictionary.\n",
      " |          custom_objects: Optional dictionary mapping names\n",
      " |              (strings) to custom classes or functions to be\n",
      " |              considered during deserialization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A model instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of improperly formatted config dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  dynamic\n",
      " |  \n",
      " |  input_spec\n",
      " |      Gets the network's input specs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of `InputSpec` instances (one per input to the model)\n",
      " |              or a single instance if the model has only one input.\n",
      " |  \n",
      " |  layers\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.training.Model:\n",
      " |  \n",
      " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, distribute=None, **kwargs)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          optimizer: String (name of optimizer) or optimizer instance.\n",
      " |              See `tf.keras.optimizers`.\n",
      " |          loss: String (name of objective function), objective function or\n",
      " |              `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n",
      " |              function is any callable with the signature\n",
      " |              `scalar_loss = fn(y_true, y_pred)`. If the model has multiple\n",
      " |              outputs, you can use a different loss on each output by passing a\n",
      " |              dictionary or a list of losses. The loss value that will be\n",
      " |              minimized by the model will then be the sum of all individual\n",
      " |              losses.\n",
      " |          metrics: List of metrics to be evaluated by the model during training\n",
      " |              and testing. Typically you will use `metrics=['accuracy']`.\n",
      " |              To specify different metrics for different outputs of a\n",
      " |              multi-output model, you could also pass a dictionary, such as\n",
      " |              `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
      " |              You can also pass a list (len = len(outputs)) of lists of metrics\n",
      " |              such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
      " |              `metrics=['accuracy', ['accuracy', 'mse']]`.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar\n",
      " |              coefficients (Python floats) to weight the loss contributions\n",
      " |              of different model outputs.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the *weighted sum* of all individual losses,\n",
      " |              weighted by the `loss_weights` coefficients.\n",
      " |              If a list, it is expected to have a 1:1 mapping\n",
      " |              to the model's outputs. If a tensor, it is expected to map\n",
      " |              output names (strings) to scalar coefficients.\n",
      " |          sample_weight_mode: If you need to do timestep-wise\n",
      " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
      " |              `None` defaults to sample-wise weights (1D).\n",
      " |              If the model has multiple outputs, you can use a different\n",
      " |              `sample_weight_mode` on each output by passing a\n",
      " |              dictionary or a list of modes.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
      " |              by sample_weight or class_weight during training and testing.\n",
      " |          target_tensors: By default, Keras will create placeholders for the\n",
      " |              model's target, which will be fed with the target data during\n",
      " |              training. If instead you would like to use your own\n",
      " |              target tensors (in turn, Keras will not expect external\n",
      " |              Numpy data for these targets at training time), you\n",
      " |              can specify them via the `target_tensors` argument. It can be\n",
      " |              a single tensor (for a single-output model), a list of tensors,\n",
      " |              or a dict mapping output names to target tensors.\n",
      " |          distribute: NOT SUPPORTED IN TF 2.0, please create and compile the\n",
      " |              model under distribution strategy scope instead of passing it to\n",
      " |              compile.\n",
      " |          **kwargs: Any additional arguments.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid arguments for\n",
      " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset.\n",
      " |            - A generator or `keras.utils.Sequence` instance.\n",
      " |            A more detailed description of unpacking behavior for iterator types\n",
      " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
      " |            for iterator-like inputs` section of `Model.fit`.\n",
      " |          y: Target data. Like the input data `x`,\n",
      " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
      " |            tensor targets, or inversely).\n",
      " |            If `x` is a dataset, generator or\n",
      " |            `keras.utils.Sequence` instance, `y` should not be specified (since\n",
      " |            targets will be obtained from the iterator/dataset).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of symbolic tensors, dataset,\n",
      " |              generators, or `keras.utils.Sequence` instances (since they generate\n",
      " |              batches).\n",
      " |          verbose: 0 or 1. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the test samples, used for weighting the loss function.\n",
      " |              You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n",
      " |              supported when `x` is a dataset, instead pass\n",
      " |              sample weights as the third element of `x`.\n",
      " |          steps: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring the evaluation round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |              If x is a `tf.data` dataset and `steps` is\n",
      " |              None, 'evaluate' will run until the dataset is exhausted.\n",
      " |              This argument is not supported with array inputs.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during evaluation.\n",
      " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      " |              input only. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up when using\n",
      " |              process-based threading. If unspecified, `workers` will default\n",
      " |              to 1. If 0, will execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children processes.\n",
      " |      \n",
      " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
      " |      `Model.fit`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: in case of invalid arguments.\n",
      " |  \n",
      " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Evaluates the model on a data generator. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use Model.evaluate, which supports generators.\n",
      " |      \n",
      " |      DEPRECATED:\n",
      " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
      " |        to use this endpoint.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False, **kwargs)\n",
      " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset. Should return a tuple\n",
      " |              of either `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      " |              or `(inputs, targets, sample weights)`.\n",
      " |            A more detailed description of unpacking behavior for iterator types\n",
      " |            (Dataset, generator, Sequence) is given below.\n",
      " |          y: Target data. Like the input data `x`,\n",
      " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
      " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
      " |            or `keras.utils.Sequence` instance, `y` should\n",
      " |            not be specified (since targets will be obtained from `x`).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of symbolic tensors, datasets,\n",
      " |              generators, or `keras.utils.Sequence` instances (since they generate\n",
      " |              batches).\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |              Note that the progress bar is not particularly useful when\n",
      " |              logged to a file, so verbose=2 is recommended when not running\n",
      " |              interactively (eg, in a production environment).\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See `tf.keras.callbacks`.\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
      " |              not supported when `x` is a dataset, generator or\n",
      " |             `keras.utils.Sequence` instance.\n",
      " |          validation_data: Data on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |              `validation_data` could be:\n",
      " |                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
      " |                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
      " |                - dataset\n",
      " |              For the first two cases, `batch_size` must be provided.\n",
      " |              For the last case, `validation_steps` could be provided.\n",
      " |          shuffle: Boolean (whether to shuffle the training data\n",
      " |              before each epoch) or str (for 'batch').\n",
      " |              'batch' is a special option for dealing with the\n",
      " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n",
      " |              supported when `x` is a dataset, generator, or\n",
      " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      " |              as the third element of `x`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              TensorFlow data tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined. If x is a\n",
      " |              `tf.data` dataset, and 'steps_per_epoch'\n",
      " |              is None, the epoch will run until the input dataset is exhausted.\n",
      " |              This argument is not supported with array inputs.\n",
      " |          validation_steps: Only relevant if `validation_data` is provided and\n",
      " |              is a `tf.data` dataset. Total number of steps (batches of\n",
      " |              samples) to draw before stopping when performing validation\n",
      " |              at the end of every epoch. If 'validation_steps' is None, validation\n",
      " |              will run until the `validation_data` dataset is exhausted. In the\n",
      " |              case of a infinite dataset, it will run into a infinite loop.\n",
      " |              If 'validation_steps' is specified and only part of the dataset\n",
      " |              will be consumed, the evaluation will start from the beginning of\n",
      " |              the dataset at each epoch. This ensures that the same validation\n",
      " |              samples are used every time.\n",
      " |          validation_freq: Only relevant if validation data is provided. Integer\n",
      " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
      " |              If an integer, specifies how many training epochs to run before a\n",
      " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
      " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
      " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
      " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      " |              input only. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up\n",
      " |              when using process-based threading. If unspecified, `workers`\n",
      " |              will default to 1. If 0, will execute the generator on the main\n",
      " |              thread.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children processes.\n",
      " |          **kwargs: Used for backwards compatibility.\n",
      " |      \n",
      " |      Unpacking behavior for iterator-like inputs:\n",
      " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
      " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      " |        yield not only features (x) but optionally targets (y) and sample weights.\n",
      " |        Keras requires that the output of such iterator-likes be unambiguous. The\n",
      " |        iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      " |        second and third elements will be used for y and sample_weight\n",
      " |        respectively. Any other type provided will be wrapped in a length one\n",
      " |        tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      " |        should still adhere to the top-level tuple structure.\n",
      " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      " |        features, targets, and weights from the keys of a single dict.\n",
      " |          A notable unsupported data type is the namedtuple. The reason is that\n",
      " |        it behaves like both an ordered datatype (tuple) and a mapping\n",
      " |        datatype (dict). So given a namedtuple of the form:\n",
      " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      " |        it is ambiguous whether to reverse the order of the elements when\n",
      " |        interpreting the value. Even worse is a tuple of the form:\n",
      " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      " |        where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      " |        and sample_weight or passed through as a single element to `x`. As a\n",
      " |        result the data processing code will simply raise a ValueError if it\n",
      " |        encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If the model was never compiled.\n",
      " |          ValueError: In case of mismatch between the provided input data\n",
      " |              and what the model expects.\n",
      " |  \n",
      " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      " |      Fits the model on data yielded batch-by-batch by a Python generator. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use Model.fit, which supports generators.\n",
      " |      \n",
      " |      DEPRECATED:\n",
      " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
      " |        this endpoint.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Retrieves the weights of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A flat list of Numpy arrays.\n",
      " |  \n",
      " |  load_weights(self, filepath, by_name=False, skip_mismatch=False)\n",
      " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
      " |      \n",
      " |      If `by_name` is False weights are loaded based on the network's\n",
      " |      topology. This means the architecture should be the same as when the weights\n",
      " |      were saved.  Note that layers that don't have weights are not taken into\n",
      " |      account in the topological ordering, so adding or removing layers is fine as\n",
      " |      long as they don't have weights.\n",
      " |      \n",
      " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
      " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
      " |      some of the layers have changed.\n",
      " |      \n",
      " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
      " |      from the TensorFlow format. Note that topological loading differs slightly\n",
      " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
      " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
      " |      TensorFlow format loads based on the object-local names of attributes to\n",
      " |      which layers are assigned in the `Model`'s constructor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, path to the weights file to load. For weight files in\n",
      " |              TensorFlow format, this is the file prefix (the same as was passed\n",
      " |              to `save_weights`).\n",
      " |          by_name: Boolean, whether to load weights by name or by topological\n",
      " |              order. Only topological loading is supported for weight files in\n",
      " |              TensorFlow format.\n",
      " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
      " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
      " |              the weight (only valid when `by_name=True`).\n",
      " |      \n",
      " |      Returns:\n",
      " |          When loading a weight file in TensorFlow format, returns the same status\n",
      " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
      " |          ops are run automatically as soon as the network is built (on first call\n",
      " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
      " |          already built).\n",
      " |      \n",
      " |          When loading weights in HDF5 format, returns `None`.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: If h5py is not available and the weight file is in HDF5\n",
      " |              format.\n",
      " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
      " |            `False`.\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input samples. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A `tf.data` dataset.\n",
      " |            - A generator or `keras.utils.Sequence` instance.\n",
      " |            A more detailed description of unpacking behavior for iterator types\n",
      " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
      " |            for iterator-like inputs` section of `Model.fit`.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of symbolic tensors, dataset,\n",
      " |              generators, or `keras.utils.Sequence` instances (since they generate\n",
      " |              batches).\n",
      " |          verbose: Verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
      " |              dataset and `steps` is None, `predict` will\n",
      " |              run until the input dataset is exhausted.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during prediction.\n",
      " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
      " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      " |              input only. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      " |              only. Maximum number of processes to spin up when using\n",
      " |              process-based threading. If unspecified, `workers` will default\n",
      " |              to 1. If 0, will execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean. Used for generator or\n",
      " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
      " |              threading. If unspecified, `use_multiprocessing` will default to\n",
      " |              `False`. Note that because this implementation relies on\n",
      " |              multiprocessing, you should not pass non-picklable arguments to\n",
      " |              the generator as they can't be passed easily to children processes.\n",
      " |      \n",
      " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
      " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
      " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
      " |      three methods.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of mismatch between the provided\n",
      " |              input data and the model's expectations,\n",
      " |              or in case a stateful model receives a number of samples\n",
      " |              that is not a multiple of the batch size.\n",
      " |  \n",
      " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Generates predictions for the input samples from a data generator. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use Model.predict, which supports generators.\n",
      " |      \n",
      " |      DEPRECATED:\n",
      " |        `Model.predict` now supports generators, so there is no longer any need\n",
      " |        to use this endpoint.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A `tf.data` dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of mismatch between given number of inputs and\n",
      " |            expectations of the model.\n",
      " |  \n",
      " |  reset_metrics(self)\n",
      " |      Resets the state of metrics.\n",
      " |  \n",
      " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |            - A `tf.data` dataset.\n",
      " |          y: Target data. Like the input data `x`,\n",
      " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
      " |            tensor targets, or inversely). If `x` is a dataset `y` should\n",
      " |            not be specified (since targets will be obtained from the iterator).\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile(). This argument is not\n",
      " |              supported when `x` is a dataset.\n",
      " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
      " |            batch. If `False`, the metrics will be statefully accumulated across\n",
      " |            batches.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid user-provided arguments.\n",
      " |  \n",
      " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          x: Input data. It could be:\n",
      " |            - A Numpy array (or array-like), or a list of arrays\n",
      " |                (in case the model has multiple inputs).\n",
      " |            - A TensorFlow tensor, or a list of tensors\n",
      " |                (in case the model has multiple inputs).\n",
      " |            - A dict mapping input names to the corresponding array/tensors,\n",
      " |                if the model has named inputs.\n",
      " |            - A `tf.data` dataset.\n",
      " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
      " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
      " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
      " |            `x` is a dataset, `y` should not be specified\n",
      " |            (since targets will be obtained from the iterator).\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |            weights to apply to the model's loss for each sample. In the case of\n",
      " |            temporal data, you can pass a 2D array with shape (samples,\n",
      " |            sequence_length), to apply a different weight to every timestep of\n",
      " |            every sample. In this case you should make sure to specify\n",
      " |            sample_weight_mode=\"temporal\" in compile(). This argument is not\n",
      " |            supported when `x` is a dataset.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
      " |            weight (float) to apply to the model's loss for the samples from this\n",
      " |            class during training. This can be useful to tell the model to \"pay\n",
      " |            more attention\" to samples from an under-represented class.\n",
      " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
      " |            batch. If `False`, the metrics will be statefully accumulated across\n",
      " |            batches.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar training loss\n",
      " |          (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: In case of invalid user-provided arguments.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.training.Model:\n",
      " |  \n",
      " |  metrics\n",
      " |      Returns the model's metrics added using `compile`, `add_metric` APIs.\n",
      " |  \n",
      " |  metrics_names\n",
      " |      Returns the model's display labels for all outputs.\n",
      " |  \n",
      " |  run_eagerly\n",
      " |      Settable attribute indicating whether the model should run eagerly.\n",
      " |      \n",
      " |      Running eagerly means that your model will be run step by step,\n",
      " |      like Python code. Your model might run slower, but it should become easier\n",
      " |      for you to debug it by stepping into individual layer calls.\n",
      " |      \n",
      " |      By default, we will attempt to compile your model to a static graph to\n",
      " |      deliver the best execution performance.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Boolean, whether the model should run eagerly.\n",
      " |  \n",
      " |  sample_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.network.Network:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid layer name or index.\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None)\n",
      " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
      " |      \n",
      " |      The savefile includes:\n",
      " |          - The model architecture, allowing to re-instantiate the model.\n",
      " |          - The model weights.\n",
      " |          - The state of the optimizer, allowing to resume training\n",
      " |              exactly where you left off.\n",
      " |      \n",
      " |      This allows you to save the entirety of the state of a model\n",
      " |      in a single file.\n",
      " |      \n",
      " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
      " |      The model returned by `load_model` is a compiled model ready to be used\n",
      " |      (unless the saved model was never compiled in the first place).\n",
      " |      \n",
      " |      Models built with the Sequential and Functional API can be saved to both the\n",
      " |      HDF5 and SavedModel formats. Subclassed models can only be saved with the\n",
      " |      SavedModel format.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, path to SavedModel or H5 file to save the model.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          include_optimizer: If True, save optimizer's state together.\n",
      " |          save_format: Either 'tf' or 'h5', indicating whether to save the model\n",
      " |              to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and\n",
      " |              'h5' in TF 1.X.\n",
      " |          signatures: Signatures to save with the SavedModel. Applicable to the\n",
      " |              'tf' format only. Please see the `signatures` argument in\n",
      " |              `tf.saved_model.save` for details.\n",
      " |          options: Optional `tf.saved_model.SaveOptions` object that specifies\n",
      " |              options for saving to SavedModel.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      from keras.models import load_model\n",
      " |      \n",
      " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
      " |      del model  # deletes the existing model\n",
      " |      \n",
      " |      # returns a compiled model\n",
      " |      # identical to the previous one\n",
      " |      model = load_model('my_model.h5')\n",
      " |      ```\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True, save_format=None)\n",
      " |      Saves all layer weights.\n",
      " |      \n",
      " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
      " |      argument.\n",
      " |      \n",
      " |      When saving in HDF5 format, the weight file has:\n",
      " |        - `layer_names` (attribute), a list of strings\n",
      " |            (ordered names of model layers).\n",
      " |        - For every layer, a `group` named `layer.name`\n",
      " |            - For every such layer group, a group attribute `weight_names`,\n",
      " |                a list of strings\n",
      " |                (ordered names of weights tensor of the layer).\n",
      " |            - For every weight in the layer, a dataset\n",
      " |                storing the weight value, named after the weight tensor.\n",
      " |      \n",
      " |      When saving in TensorFlow format, all objects referenced by the network are\n",
      " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
      " |      instances or `Optimizer` instances assigned to object attributes. For\n",
      " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
      " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
      " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
      " |      `Layer` instances must be assigned to object attributes, typically in the\n",
      " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
      " |      `tf.keras.Model` for details.\n",
      " |      \n",
      " |      While the formats are the same, do not mix `save_weights` and\n",
      " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
      " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
      " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
      " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
      " |      `save_weights` for training checkpoints.\n",
      " |      \n",
      " |      The TensorFlow format matches objects and variables by starting at a root\n",
      " |      object, `self` for `save_weights`, and greedily matching attribute\n",
      " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
      " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
      " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
      " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
      " |      the `Model`'s variables. See the [guide to training\n",
      " |      checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n",
      " |      on the TensorFlow format.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          filepath: String, path to the file to save the weights to. When saving\n",
      " |              in TensorFlow format, this is the prefix used for checkpoint files\n",
      " |              (multiple files are generated). Note that the '.h5' suffix causes\n",
      " |              weights to be saved in HDF5 format.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
      " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
      " |              `None` defaults to 'tf'.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: If h5py is not available when attempting to save in HDF5\n",
      " |              format.\n",
      " |          ValueError: For invalid/unknown format arguments.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided,\n",
      " |              defaults to `[.33, .55, .67, 1.]`.\n",
      " |          print_fn: Print function to use. Defaults to `print`.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if `summary()` is called before the model is built.\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `json.dumps()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  to_yaml(self, **kwargs)\n",
      " |      Returns a yaml string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a yaml save file, use\n",
      " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
      " |      \n",
      " |      `custom_objects` should be a dictionary mapping\n",
      " |      the names of custom losses / layers / etc to the corresponding\n",
      " |      functions / classes.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `yaml.dump()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A YAML string.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ImportError: if yaml module is not found.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.network.Network:\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  state_updates\n",
      " |      Returns the `updates` from all layers that are stateful.\n",
      " |      \n",
      " |      This is useful for separating training updates and\n",
      " |      state updates, e.g. when we need to update a layer's internal state\n",
      " |      during prediction.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Actvity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this module as passed or determined in the ctor.\n",
      " |      \n",
      " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
      " |      parent module names.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      ```\n",
      " |      class MyModule(tf.Module):\n",
      " |        @tf.Module.with_name_scope\n",
      " |        def __call__(self, x):\n",
      " |          if not hasattr(self, 'w'):\n",
      " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
      " |          return tf.matmul(x, self.w)\n",
      " |      ```\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      ```\n",
      " |      mod = MyModule()\n",
      " |      mod(tf.ones([8, 32]))\n",
      " |      # ==> <tf.Tensor: ...>\n",
      " |      mod.w\n",
      " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      ```\n",
      " |      a = tf.Module()\n",
      " |      b = tf.Module()\n",
      " |      c = tf.Module()\n",
      " |      a.b = b\n",
      " |      b.c = c\n",
      " |      assert list(a.submodules) == [b, c]\n",
      " |      assert list(b.submodules) == [c]\n",
      " |      assert list(c.submodules) == []\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets, training=True)\n",
    "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6, Initial Loss: 1.3073917627334595\n",
      "Step: 7,         Loss: 1.2232798337936401\n"
     ]
    }
   ],
   "source": [
    "loss_value, grads = grad(model, features, labels)\n",
    "\n",
    "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss_value.numpy()))\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss(model, features, labels, training=True).numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 1 0 1 1 2 1 0 2 0 1 2 2 0 1 0 0 1 2 0 2 0 2 1 2 0 1 2 2 0 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor([2 0 2 0 2 0 2 0 0 0 2 2 0 2 1 2 2 1 0 2 1 1 0 0 1 2 0 2 0 2 1 2], shape=(32,), dtype=int32)\n",
      "tf.Tensor([2 1 0 0 1 0 1 0 2 2 0 2 2 2 2 2 1 0 0 2 2 0 1 2 1 1 0 1 2 1 0 0], shape=(32,), dtype=int32)\n",
      "tf.Tensor([1 1 0 0 0 1 2 0 2 1 2 1 0 0 2 1 1 2 1 0 1 2 0 1 2 0 1 2 0 1 0 2], shape=(32,), dtype=int32)\n",
      "tf.Tensor([1 2 0 1 0 1 2 2 1 1 0 1 1 2 0 0 1 0 1 2 2 0 2 0 2 1 2 2 2 1 0 0], shape=(32,), dtype=int32)\n",
      "tf.Tensor([1 2 2 2 2 0 0 0 0 0 0 2 2 0 0 2 0 0 2 1 0 2 1 1 1 0 2 1 2 2 2 0], shape=(32,), dtype=int32)\n",
      "tf.Tensor([1 0 0 1 0 0 2 1 0 0 0 2 1 2 0 1 0 2 1 0 2 2 2 2 1 2 0 0 1 2 1 2], shape=(32,), dtype=int32)\n",
      "tf.Tensor([0 0 1 1 1 1 2 0 1 2 1 1 1 0 2 2], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 1.027, Accuracy: 66.250%\n",
      "Epoch 050: Loss: 0.244, Accuracy: 97.500%\n",
      "Epoch 100: Loss: 0.128, Accuracy: 98.333%\n",
      "Epoch 150: Loss: 0.096, Accuracy: 99.167%\n",
      "Epoch 200: Loss: 0.084, Accuracy: 97.917%\n"
     ]
    }
   ],
   "source": [
    "## Note: Rerunning this cell uses the same model variables\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 201\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "  # Training loop - using batches of 32\n",
    "  for x, y in train_dataset:\n",
    "    # Optimize the model\n",
    "    loss_value, grads = grad(model, x, y)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Track progress\n",
    "    epoch_loss_avg(loss_value)  # Add current batch loss\n",
    "    # Compare predicted label to actual label\n",
    "    # training=True is needed only if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    epoch_accuracy(y, model(x, training=True))\n",
    "\n",
    "  # End epoch\n",
    "  train_loss_results.append(epoch_loss_avg.result())\n",
    "  train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "  if epoch % 50 == 0:\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
